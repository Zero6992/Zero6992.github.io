{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-landscape/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.css","path":"fancybox/jquery.fancybox.min.css","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.js","path":"fancybox/jquery.fancybox.min.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/js/jquery-3.4.1.min.js","path":"js/jquery-3.4.1.min.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.woff2","path":"css/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/assets/fonts.css","path":"assets/fonts.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/assets/odometer-theme-minimal.css","path":"assets/odometer-theme-minimal.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/all.min.css","path":"fontawesome/all.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/duotone.min.css","path":"fontawesome/duotone.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/brands.min.css","path":"fontawesome/brands.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/fontawesome.min.css","path":"fontawesome/fontawesome.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/light.min.css","path":"fontawesome/light.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/regular.min.css","path":"fontawesome/regular.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/sharp-solid.min.css","path":"fontawesome/sharp-solid.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/solid.min.css","path":"fontawesome/solid.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/svg-with-js.min.css","path":"fontawesome/svg-with-js.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/thin.min.css","path":"fontawesome/thin.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/v4-font-face.min.css","path":"fontawesome/v4-font-face.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/v4-shims.min.css","path":"fontawesome/v4-shims.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/v5-font-face.min.css","path":"fontawesome/v5-font-face.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fonts/OPTIMA.woff","path":"fonts/OPTIMA.woff","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fonts/OPTIMA_B.woff","path":"fonts/OPTIMA_B.woff","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fonts/Optima Medium.woff","path":"fonts/Optima Medium.woff","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/fonts/Optima_Italic.woff","path":"fonts/Optima_Italic.woff","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/loading.svg","path":"images/loading.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/redefine-avatar.svg","path":"images/redefine-avatar.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/redefine-favicon.svg","path":"images/redefine-favicon.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/redefine-logo.svg","path":"images/redefine-logo.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/redefine-logo.webp","path":"images/redefine-logo.webp","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/wallhaven-wqery6-dark.webp","path":"images/wallhaven-wqery6-dark.webp","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/wallhaven-wqery6-light.webp","path":"images/wallhaven-wqery6-light.webp","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-brands-400.ttf","path":"webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-brands-400.woff2","path":"webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-duotone-900.ttf","path":"webfonts/fa-duotone-900.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-duotone-900.woff2","path":"webfonts/fa-duotone-900.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-light-300.ttf","path":"webfonts/fa-light-300.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-light-300.woff2","path":"webfonts/fa-light-300.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-regular-400.ttf","path":"webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-regular-400.woff2","path":"webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-sharp-solid-900.ttf","path":"webfonts/fa-sharp-solid-900.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-sharp-solid-900.woff2","path":"webfonts/fa-sharp-solid-900.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-solid-900.ttf","path":"webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-thin-100.ttf","path":"webfonts/fa-thin-100.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-thin-100.woff2","path":"webfonts/fa-thin-100.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-v4compatibility.ttf","path":"webfonts/fa-v4compatibility.ttf","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-v4compatibility.woff2","path":"webfonts/fa-v4compatibility.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-solid-900.woff2","path":"webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/common/animated.styl","path":"css/common/animated.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/common/basic.styl","path":"css/common/basic.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/common/markdown.styl","path":"css/common/markdown.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/common/variables.styl","path":"css/common/variables.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/common/redefine-theme.styl","path":"css/common/redefine-theme.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/article-content.styl","path":"css/layout/article-content.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/archive-content.styl","path":"css/layout/archive-content.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/category-content.styl","path":"css/layout/category-content.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/category-list.styl","path":"css/layout/category-list.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/page.styl","path":"css/layout/page.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/home-content.styl","path":"css/layout/home-content.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/home-sidebar.styl","path":"css/layout/home-sidebar.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/tag-content.styl","path":"css/layout/tag-content.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/layouts/odometer.min.js","path":"js/layouts/odometer.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/layouts/navbarShrink.js","path":"js/layouts/navbarShrink.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/layouts/lazyload.js","path":"js/layouts/lazyload.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/layouts/toc.js","path":"js/layouts/toc.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/APlayer.min.js","path":"js/libs/APlayer.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/APlayer.min.js.map","path":"js/libs/APlayer.min.js.map","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/anime.min.js","path":"js/libs/anime.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/Typed.min.js","path":"js/libs/Typed.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/mermaid.min.js.map","path":"js/libs/mermaid.min.js.map","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/mermaid.min.js","path":"js/libs/mermaid.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/plugins/aplayer.js","path":"js/plugins/aplayer.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/pjax.min.js","path":"js/libs/pjax.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/plugins/typed.js","path":"js/plugins/typed.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/plugins/mermaid.js","path":"js/plugins/mermaid.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/plugins/tabs.js","path":"js/plugins/tabs.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/codeBlock.js","path":"js/tools/codeBlock.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/localSearch.js","path":"js/tools/localSearch.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/lightDarkSwitch.js","path":"js/tools/lightDarkSwitch.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/runtime.js","path":"js/tools/runtime.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/tocToggle.js","path":"js/tools/tocToggle.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/scrollTopBottom.js","path":"js/tools/scrollTopBottom.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/common/codeblock/code-block.styl","path":"css/common/codeblock/code-block.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/common/codeblock/code-theme.styl","path":"css/common/codeblock/code-theme.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/css/common/codeblock/highlight.styl","path":"css/common/codeblock/highlight.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/zero.svg","path":"images/zero.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-redefine/source/images/zero2.webp","path":"images/zero2.webp","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1680768138258},{"_id":"node_modules/hexo-theme-landscape/package.json","hash":"9a94875cbf4c27fbe2e63da0496242addc6d2876","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/README.md","hash":"d2772ece6d4422ccdaa0359c3e07588834044052","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/_config.yml","hash":"b608c1f1322760dce9805285a602a95832730a2e","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/en.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/hu.yml","hash":"284d557130bf54a74e7dcef9d42096130e4d9550","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/it.yml","hash":"89b7d91306b2c1a0f3ac023b657bf974f798a1e8","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/mn.yml","hash":"2e7523951072a9403ead3840ad823edd1084c116","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/tr.yml","hash":"a1cdbfa17682d7a971de8ab8588bf57c74224b5b","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/zh-CN.yml","hash":"1efd95774f401c80193eac6ee3f1794bfe93dc5a","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/layout.ejs","hash":"0d1765036e4874500e68256fedb7470e96eeb6ee","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/scripts/fancybox.js","hash":"c857d7a5e4a5d71c743a009c5932bf84229db428","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/archive.ejs","hash":"7cb70a7a54f8c7ae49b10d1f37c0a9b74eab8826","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/footer.ejs","hash":"3656eb692254346671abc03cb3ba1459829e0dce","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/after-footer.ejs","hash":"414914ebb159fac1922b056b905e570ac7521925","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/article.ejs","hash":"dfd555c00e85ffc4207c88968d12b219c1f086ec","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/gauges-analytics.ejs","hash":"21a1e2a3907d1a3dad1cd0ab855fe6735f233c74","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/google-analytics.ejs","hash":"2ea7442ea1e1a8ab4e41e26c563f58413b59a3d0","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/header.ejs","hash":"c1acd247e14588cdf101a69460cb8319c18cd078","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/head.ejs","hash":"f215d92a882247a7cc5ea80b241bedfcec0ea6ca","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/recent_posts.ejs","hash":"60c4b012dcc656438ff59997e60367e5a21ab746","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_variables.styl","hash":"581b0cbefdaa5f894922133989dd2d3bf71ded79","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/style.styl","hash":"9c451e5efd72c5bb8b56e8c2b94be731e99db05b","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/js/script.js","hash":"998ed4c5b147e1299bf62beebf33514474f28112","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/date.ejs","hash":"f1458584b679545830b75bef2526e2f3eb931045","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/title.ejs","hash":"4d7e62574ddf46de9b41605fe3140d77b5ddb26d","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/article.styl","hash":"80759482d07063c091e940f964a1cf6693d3d406","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1680767885267},{"_id":"node_modules/hexo-theme-landscape/source/css/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1680767885267},{"_id":"node_modules/hexo-theme-redefine/.editorconfig","hash":"a1c91f0a086bf92fddb02ccf23578ec2b51c099c","modified":1680768379444},{"_id":"node_modules/hexo-theme-redefine/DONATION.md","hash":"bcf3d90e85af64c7fdf489f4307f31ec2374b225","modified":1680768381106},{"_id":"node_modules/hexo-theme-redefine/CONTRIBUTING.md","hash":"c6ef834f602b7ae02e2863a2a857e0ce8d392f15","modified":1680768381106},{"_id":"node_modules/hexo-theme-redefine/CODE_OF_CONDUCT.md","hash":"61a6276ef54989b7a1325f3ecb3183a4dfdf50cb","modified":1680768381105},{"_id":"node_modules/hexo-theme-redefine/README.md","hash":"5a527c5f22aea610100217c844f47e6845bbec6a","modified":1680768381111},{"_id":"node_modules/hexo-theme-redefine/package.json","hash":"5e9e9f1d692885ab937305d9763f2c53af3c5b94","modified":1680768380576},{"_id":"node_modules/hexo-theme-redefine/_config.yml","hash":"e81a7e6feba9d053f1e2597dc122301ac6d3be59","modified":1680768385148},{"_id":"node_modules/hexo-theme-redefine/README_zh-CN.md","hash":"3f4ca00e9dcc82d59d02d8493ad8f90b6bb1520c","modified":1680768381107},{"_id":"node_modules/hexo-theme-redefine/README_zh-TW.md","hash":"d6aa9719fd297e7176276df4e0e326dbad94f9a9","modified":1680768381107},{"_id":"node_modules/hexo-theme-redefine/languages/en.yml","hash":"fbb40b3d982c44d6e4eec412b3e14f0c112f7c6c","modified":1680768385149},{"_id":"node_modules/hexo-theme-redefine/LICENSE","hash":"a2f9ffbf32eeb6284afa81bc4fb4c27b80d044e9","modified":1680768379447},{"_id":"node_modules/hexo-theme-redefine/layout/404.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1680768379931},{"_id":"node_modules/hexo-theme-redefine/languages/zh-TW.yml","hash":"06cc96d3963e773a42c6d82db0033dc51e1e7894","modified":1680768385149},{"_id":"node_modules/hexo-theme-redefine/languages/zh-CN.yml","hash":"46b054405752fce00de47a89da528bbb10e1a95f","modified":1680768385149},{"_id":"node_modules/hexo-theme-redefine/layout/category-content.ejs","hash":"6ccd36ecd0ae953a516903e96345134d3a0dfcfe","modified":1680768379933},{"_id":"node_modules/hexo-theme-redefine/layout/article-content.ejs","hash":"f29c8b96804593ededf1b09b8e0d8d8ae3be4ee1","modified":1680768379932},{"_id":"node_modules/hexo-theme-redefine/layout/category-list.ejs","hash":"9bcc05b89fbfa2d77cbcbafa2082d137f1a9a8d8","modified":1680768379934},{"_id":"node_modules/hexo-theme-redefine/layout/archive-content.ejs","hash":"5174216653be703b5a76f38d7adf9356cfc7081e","modified":1680768379932},{"_id":"node_modules/hexo-theme-redefine/layout/archive.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1680768379932},{"_id":"node_modules/hexo-theme-redefine/layout/index.ejs","hash":"f5fee4e079a9c2c23059ebde4cd89ec723e310c6","modified":1680768379938},{"_id":"node_modules/hexo-theme-redefine/layout/tag-content.ejs","hash":"c09d62d899ef4127d32a75a3486fa0863e29ea17","modified":1680768379941},{"_id":"node_modules/hexo-theme-redefine/layout/category.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1680768379934},{"_id":"node_modules/hexo-theme-redefine/layout/page.ejs","hash":"73692a05ce3c6bcc9d6edcb2f43ea97228a62673","modified":1680768379940},{"_id":"node_modules/hexo-theme-redefine/layout/tag.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1680768379941},{"_id":"node_modules/hexo-theme-redefine/layout/layout.ejs","hash":"e13ebcba0323e0641457d04009c6b548b914ef3d","modified":1680768379939},{"_id":"node_modules/hexo-theme-redefine/.github/ISSUE_TEMPLATE/feature-request--english-version-.md","hash":"243dbc4cf0678ca4f89fc2559eab3ace9fa881a7","modified":1680768381107},{"_id":"node_modules/hexo-theme-redefine/scripts/use-source-data.js","hash":"97b3078f5d28a99dca73b8db52bc892e65536c2b","modified":1680768380575},{"_id":"node_modules/hexo-theme-redefine/layout/tags.ejs","hash":"8456f112fc12bbb1c83cd190d0ce83ee474bd297","modified":1680768379942},{"_id":"node_modules/hexo-theme-redefine/.github/ISSUE_TEMPLATE/bug-report--english-version-.md","hash":"6b7cd67ce8911fdc0863da1b54e404514c7e85e5","modified":1680768381104},{"_id":"node_modules/hexo-theme-redefine/.github/ISSUE_TEMPLATE/功能建议--中文版本-.md","hash":"ef69a31bcdbe2f749fd4990a4a85d51c300cd2dd","modified":1680768381112},{"_id":"node_modules/hexo-theme-redefine/.github/ISSUE_TEMPLATE/bug-提交--中文版本-.md","hash":"c77b4ab57ed2a7ddbd86e2ba4bd5dba850d394c6","modified":1680768381105},{"_id":"node_modules/hexo-theme-redefine/.github/workflows/aliyun_cdn.yml","hash":"09693748d95f126abe95582152d1c364bc82b4bb","modified":1680768385148},{"_id":"node_modules/hexo-theme-redefine/.github/workflows/npm-publish.yml","hash":"2b1771ae2a71e3c148b09d507f9be47d04e6e623","modified":1680768385149},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/404-template.ejs","hash":"7729caa836a1c2208860cada53d674e9e9ff61f3","modified":1680768379931},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/home-content.ejs","hash":"15276322cd0b3922c1d66282ea7d59a3833b865b","modified":1680768379937},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/footer.ejs","hash":"782fb432998aa2fcbbfdb204ad1e71f2ef4d846b","modified":1680768379936},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/head.ejs","hash":"a061a71583e243ea25b01307e596a04340e68490","modified":1680768379937},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/home-banner.ejs","hash":"d00e610fda927e76419d4bbfabf6ac139dfc8bb3","modified":1680768379937},{"_id":"node_modules/hexo-theme-redefine/layout/_meta/article-copyright.ejs","hash":"b57fb813801d33b2ec50dd1ed0c2d5e929fd19b1","modified":1680768379933},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/home-sidebar.ejs","hash":"f9e2e322131f1e9ee0fe6b893012c0994cad06da","modified":1680768379938},{"_id":"node_modules/hexo-theme-redefine/layout/_meta/article-info.ejs","hash":"32901bc97d3a3b713e4f92697767412edc55f072","modified":1680768379933},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/page-template.ejs","hash":"841863bd2afa52ca3d1be369444c6a1d11a89c93","modified":1680768379940},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/navbar.ejs","hash":"789b4f6fe960733143778e658123d33a2105323f","modified":1680768379940},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/scripts.ejs","hash":"f866c89c4761c1e05f658eff0bf827e41e3867df","modified":1680768379941},{"_id":"node_modules/hexo-theme-redefine/layout/_meta/home-article.ejs","hash":"c04f174e8ec5ed5a18906d47aa45c7af29ee9ab2","modified":1680768379937},{"_id":"node_modules/hexo-theme-redefine/layout/_plugins/aplayer.ejs","hash":"c8f275cdbbe14b4673c69f7c1463b1f830ec8e40","modified":1680768379931},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/essays.ejs","hash":"710193dbb155fa0b5999202709043746d22aa712","modified":1680768379936},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/friends-link.ejs","hash":"3787e5c947e04e2cac3ed31d19380f4aeeba8641","modified":1680768379936},{"_id":"node_modules/hexo-theme-redefine/layout/_plugins/pjax.ejs","hash":"08d1a95ce23bbe42b4a6d8ae30e7adc770b0eca6","modified":1680768379941},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/archive-list.ejs","hash":"37b332733e393cb7bbeedbb15e02c86205d35755","modified":1680768379932},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/post-tools.ejs","hash":"1803ba188f4b3ed63c6993002b210fdc690cfd76","modified":1680768379941},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/progress-bar.ejs","hash":"cc7f8455e7559b59e1c15eacc5576f21b1561060","modified":1680768379941},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/image-viewer.ejs","hash":"beb3dec5e08856a29d585fe0eb94a8a6ddade4dd","modified":1680768379938},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/side-tools.ejs","hash":"61cadc3d25555430b5fe3f2c1255b87d2d7d08b7","modified":1680768379941},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/paginator.ejs","hash":"a237e7b858e0cf32ca67553d540f7c5f7239a8d0","modified":1680768379941},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/toc.ejs","hash":"46ef94dca9be59d163f6b4cba16e1695e8020bc3","modified":1680768379942},{"_id":"node_modules/hexo-theme-redefine/scripts/events/welcome.js","hash":"10e2d223140bc547de14f344e046ab8b0aa48bf0","modified":1680768380576},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/tagcloud.ejs","hash":"ff5a94bb817eefb1a4af05e565541900a7702302","modified":1680768379942},{"_id":"node_modules/hexo-theme-redefine/scripts/helpers/articleRecommend.js","hash":"629891889170c701aeb5fdc1b32cd57928c0303b","modified":1680768380048},{"_id":"node_modules/hexo-theme-redefine/scripts/events/404.js","hash":"169245d7b2af1ff401cf76e718c28b4e38f637c7","modified":1680768379943},{"_id":"node_modules/hexo-theme-redefine/scripts/helpers/seoGenerate.js","hash":"e0059a60cc5978be6792dcf795a2ece3a509f41a","modified":1680768380571},{"_id":"node_modules/hexo-theme-redefine/scripts/helpers/configExport.js","hash":"04abcb4645c1a96ecb072f3f4743b68a93b6136e","modified":1680768380050},{"_id":"node_modules/hexo-theme-redefine/scripts/helpers/themeHelpers.js","hash":"f2e4edade79f6b3723665135a12748ddca37a3f2","modified":1680768380571},{"_id":"node_modules/hexo-theme-redefine/layout/_widgets/local-search.ejs","hash":"72c9acca0280f0e63e6a5d31618dd2707bdbec3f","modified":1680768379939},{"_id":"node_modules/hexo-theme-redefine/scripts/filters/lazyloadHandle.js","hash":"f8ae44311e6463e887c07a41910ebade8766bac2","modified":1680768380051},{"_id":"node_modules/hexo-theme-redefine/scripts/helpers/autoCanonical.js","hash":"f3ea74759129c71041371a1d77c687eb6aa88d3a","modified":1680768380049},{"_id":"node_modules/hexo-theme-redefine/scripts/modules/btns.js","hash":"a91492e772287114527a3fbc85f0c7c1c1b15eb3","modified":1680768380049},{"_id":"node_modules/hexo-theme-redefine/scripts/filters/linkHandle.js","hash":"a92ed888dad340809ee558020e0bb9f41e6413ee","modified":1680768380051},{"_id":"node_modules/hexo-theme-redefine/scripts/modules/btn.js","hash":"3e4c1b01a1f922f712895e9ac06c030231b35cf2","modified":1680768380049},{"_id":"node_modules/hexo-theme-redefine/scripts/modules/note.js","hash":"7c529ffe108a04a03ae6667074f69c5e62be8c54","modified":1680768380567},{"_id":"node_modules/hexo-theme-redefine/scripts/modules/folding.js","hash":"cfa6646c1350c557430149bd52578c6cac59063b","modified":1680768380050},{"_id":"node_modules/hexo-theme-redefine/source/assets/fonts.css","hash":"39f2b7042486b4a1df2f4304a6569b21559804e6","modified":1680768379924},{"_id":"node_modules/hexo-theme-redefine/scripts/modules/tabs.js","hash":"7ad85a4de6918a9b646cc083704d4fd8a387b98d","modified":1680768380571},{"_id":"node_modules/hexo-theme-redefine/scripts/modules/note-large.js","hash":"88a53682ebb71ff2ce1d08205f5eb22b837acd44","modified":1680768380567},{"_id":"node_modules/hexo-theme-redefine/source/css/style.styl","hash":"35eadf997494cfd0b04ed44fc933086058a8189a","modified":1680768381136},{"_id":"node_modules/hexo-theme-redefine/source/assets/odometer-theme-minimal.css","hash":"793c31feb38e241a5ff6ecc5e65e61751507d728","modified":1680768379925},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/brands.min.css","hash":"5119c35bbd273d2ea2501997827f6d644da93164","modified":1680768379604},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/light.min.css","hash":"18a176eb2486db4e938e568083fa578b641ccc83","modified":1680768379925},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/solid.min.css","hash":"e727feabedb171fb0b398151870d7739ef4e2432","modified":1680768379926},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/sharp-solid.min.css","hash":"7d1a13c6b8178b29f5bcb1a639a1998fc9de16c4","modified":1680768379926},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/regular.min.css","hash":"d27c4fed54dacff688010e51f611d950dd6e0aca","modified":1680768379926},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/v4-shims.min.css","hash":"96f037860b8e9b59af8e47571ea98791616dbfd8","modified":1680768379928},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/svg-with-js.min.css","hash":"f4c08e37c4d44b3ed7a3f377d1cb5d7fca4d04bc","modified":1680768379927},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/thin.min.css","hash":"3d46be31379b07ffb24d69c6c915725eaa2e89e9","modified":1680768379927},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/v5-font-face.min.css","hash":"260ea7a5e0b89963a5dcf2600a4decdf0f408f3f","modified":1680768379930},{"_id":"node_modules/hexo-theme-redefine/source/fonts/OPTIMA.woff","hash":"ddefd7389a3aaf00d50e2ecc99f71eefe853c4b3","modified":1680768383754},{"_id":"node_modules/hexo-theme-redefine/source/fonts/Optima Medium.woff","hash":"f18b71ba83f68f1478b0af64cf76a99971c4fd9a","modified":1680768383600},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/v4-font-face.min.css","hash":"0be987628c8b485b39f064da41c90f15c0596c1f","modified":1680768379928},{"_id":"node_modules/hexo-theme-redefine/source/fonts/OPTIMA_B.woff","hash":"129a1b30e7b829b9894245b3c99e1c6c016e4147","modified":1680768383602},{"_id":"node_modules/hexo-theme-redefine/source/fonts/Optima_Italic.woff","hash":"7200bef189fb05f741555e734908df59d0f20e2f","modified":1680768383752},{"_id":"node_modules/hexo-theme-redefine/source/images/redefine-avatar.svg","hash":"d0d20061bda08894a82d7691b660be7c6aaa0608","modified":1680768381139},{"_id":"node_modules/hexo-theme-redefine/source/images/redefine-logo.svg","hash":"472776b6f013aad44706fee6c94201c96ee83932","modified":1680768381139},{"_id":"node_modules/hexo-theme-redefine/source/images/loading.svg","hash":"364550d66fb15b740c038da4780ff4c78c500c70","modified":1680768381138},{"_id":"node_modules/hexo-theme-redefine/source/images/redefine-favicon.svg","hash":"420f930a1df64a3c4391ff80326a8a2d7d5a6418","modified":1680768381139},{"_id":"node_modules/hexo-theme-redefine/source/images/redefine-logo.webp","hash":"0a07e3fb6d9125dee44798c8c110187b16fb42a9","modified":1680768383402},{"_id":"node_modules/hexo-theme-redefine/source/images/wallhaven-wqery6-light.webp","hash":"d25389973d0359b78f1e9c74a850ef425690ba40","modified":1680768383598},{"_id":"node_modules/hexo-theme-redefine/source/images/wallhaven-wqery6-dark.webp","hash":"d0066e0b025ae748448a3d6a96165d45a55d2f22","modified":1680768383434},{"_id":"node_modules/hexo-theme-redefine/source/js/utils.js","hash":"ef9aa7f244d8807882f27f0b1b2407fb6a459920","modified":1680768380575},{"_id":"node_modules/hexo-theme-redefine/source/js/main.js","hash":"c92deaee4a6f03ae92061be7935181c0011007ca","modified":1680768380053},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-v4compatibility.ttf","hash":"b54531dd09c5089eb93b12ce8f90ff521855ff8a","modified":1680768383394},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-v4compatibility.woff2","hash":"d02b1adc81fd5bec023e25a7770779b99d6dd742","modified":1680768385148},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/comments/comment.ejs","hash":"6e37398dfd21ff57305a4903d825501756c6b57e","modified":1680768379934},{"_id":"node_modules/hexo-theme-redefine/source/css/common/animated.styl","hash":"b45d84487321afa6a294e6fb1c179eb65901b0d3","modified":1680768381119},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/comments/gitalk.ejs","hash":"6113dece4b9477b8151f68f057626d22debc4826","modified":1680768379937},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/comments/twikoo.ejs","hash":"78806c56ec1bbd9fa35b9a5c2cd245744822362a","modified":1680768379942},{"_id":"node_modules/hexo-theme-redefine/source/css/common/basic.styl","hash":"3ca5de789baea71511b510f6462ddbcaf2d0fad9","modified":1680768381125},{"_id":"node_modules/hexo-theme-redefine/source/css/common/markdown.styl","hash":"faae7b16145ee122f5f9f9bba6737cdcc265a951","modified":1680768381133},{"_id":"node_modules/hexo-theme-redefine/layout/_partials/comments/waline.ejs","hash":"3c4b7f801c2ae58b4678fbe6f4ea737113183e40","modified":1680768379942},{"_id":"node_modules/hexo-theme-redefine/source/css/common/redefine-theme.styl","hash":"e36c2cb2bc9726cf41fca5473bd458f5176ce6d2","modified":1680768381135},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/article-content.styl","hash":"03434b35827a537d9bfa75a5d13cf7420fdadeac","modified":1680768381123},{"_id":"node_modules/hexo-theme-redefine/source/css/common/variables.styl","hash":"6505e0f18fe4a0f22a600aedf32acb54a6ae17ee","modified":1680768381137},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/category-content.styl","hash":"aa58866edf338b395934c061c61af9cdfc3ab35c","modified":1680768381125},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/category-list.styl","hash":"6f9519aa18e4b49a8be82e9f5bf4785533aef16b","modified":1680768381125},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/archive-content.styl","hash":"f8df2e4c4829bf081fa49acc0832fec0d07bdeb9","modified":1680768381122},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/page.styl","hash":"fb8eea01d0afa3c3a1c8da66c2a88d12ce0d3639","modified":1680768381135},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/home-sidebar.styl","hash":"dbf9cecd74b7e860ad8e58223dd98e300f435aeb","modified":1680768381133},{"_id":"node_modules/hexo-theme-redefine/source/js/layouts/odometer.min.js","hash":"fe5beb60997c84ab2f91b54535c4221443cdd21f","modified":1680768380568},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/home-content.styl","hash":"001b0f0ef10618c9ae9aaf2450e248e4c279f3d4","modified":1680768381131},{"_id":"node_modules/hexo-theme-redefine/source/js/layouts/navbarShrink.js","hash":"e5ba40c5e634f6e02d846e3eac13da198a20c370","modified":1680768380567},{"_id":"node_modules/hexo-theme-redefine/source/js/layouts/lazyload.js","hash":"1e30f2cf7c05d29682aa9c006e83165a0d94a87f","modified":1680768380051},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/tag-content.styl","hash":"1fbb0baa9ce21f3b0d1ddfad1d14641845d65f18","modified":1680768381136},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1680768380048},{"_id":"node_modules/hexo-theme-redefine/source/js/layouts/toc.js","hash":"6ab4a945013ccc55381460cb8d2aab9dd903c513","modified":1680768380573},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/Typed.min.js","hash":"e8ce2b674a637b0c0396a3106c1aedf10186249c","modified":1680768380574},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1680768379943},{"_id":"node_modules/hexo-theme-redefine/source/js/plugins/aplayer.js","hash":"d0d1960fcacf55b9ccc7d80264c239e4cfb161e7","modified":1680768379944},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/pjax.min.js","hash":"57ec40cb4898d7ba74a03603d608af378e5431f8","modified":1680768380569},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/lightDarkSwitch.js","hash":"3d94ee2ff3c5d38eab6b7b6d0927b495f02da0ce","modified":1680768380051},{"_id":"node_modules/hexo-theme-redefine/source/js/plugins/mermaid.js","hash":"eef31d84b2b7083f2c52d5796fd8d520b3f028b2","modified":1680768380054},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/localSearch.js","hash":"6ed38d07474faede65ee10a5c9fb3cb75ee7207c","modified":1680768380052},{"_id":"node_modules/hexo-theme-redefine/source/js/plugins/tabs.js","hash":"2c7d13003edd19c4d87318688b8e4c4a2e8258be","modified":1680768380571},{"_id":"node_modules/hexo-theme-redefine/source/js/plugins/typed.js","hash":"8f7189ccfffa30390d61f1173126864cc489a420","modified":1680768380574},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/codeBlock.js","hash":"abb663661f42b312cb9bb3d6a29bbedcac6963c9","modified":1680768380050},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/runtime.js","hash":"aa0fc65ae96e5e057169cf80ad9cfedb5c625626","modified":1680768380570},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/tocToggle.js","hash":"b8993268199f954c0f95a8b656857d4dd2f5e3d6","modified":1680768380574},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_modules/folding.styl","hash":"ba975f0d64c7298fec228f51c72bc34abb250d2e","modified":1680768381130},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_modules/aplayer.styl","hash":"f4bf3af408034a4949a60038b8d1e096335bc187","modified":1680768381120},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_modules/buttons.styl","hash":"04d2f3834821fa17e0dca0ef5eb936b229a7bdc6","modified":1680768381125},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_modules/tabs.styl","hash":"522d7a409005477883bd6272be3c74cc60acf4e8","modified":1680768381136},{"_id":"node_modules/hexo-theme-redefine/source/js/tools/scrollTopBottom.js","hash":"9877543c1f17a5647055e250e9cf950bed9e8599","modified":1680768380570},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/404.styl","hash":"2195e6e3fb7cfb3d63f053bc4d3d40e64a442053","modified":1680768381118},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_modules/notes.styl","hash":"2a05e7254943073af6556e1a44bef392e3a368fd","modified":1680768381134},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/article-meta-info.styl","hash":"6d870476b99cffd18d9e3aeae2633d6485678709","modified":1680768381124},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/home-banner.styl","hash":"d8ff00df4c33a0818f796eb53ecca9b4b5ea5347","modified":1680768381131},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/article-copyright-info.styl","hash":"47adec5398548dd306e6baa7e5a2ecef05553288","modified":1680768381124},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/footer.styl","hash":"aca7e039b827e3ea6264ffd9ee37c800a7c6f86a","modified":1680768381130},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/archive-list.styl","hash":"a2e8b6ef9d4295de1ea9ea97ce75e4f060a02be1","modified":1680768381122},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/navbar.styl","hash":"a0ee7f444d46157c7e22d966bfaf835f13601003","modified":1680768381134},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/image-viewer.styl","hash":"fe4e168a1968b3f9b33d0292944590e76809e437","modified":1680768381133},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/progress-bar.styl","hash":"9b3b0d5e544b78fd79e07b05352a26e0cf9e392c","modified":1680768381135},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/paginator.styl","hash":"e3c07330f426f283e04f256b27cf0abd22cf3ca0","modified":1680768381135},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/local-search.styl","hash":"575dba66a4b2914cd6e4627ab9f6369bc08c4d99","modified":1680768381133},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/post-tools.styl","hash":"029b95e120266e71f3c82514488c0021d0b72971","modified":1680768381135},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/page-template.styl","hash":"330c8d519350240a47ffb7dd330bb7a1a070988e","modified":1680768381134},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/side-tools.styl","hash":"7d1080951501182674cc87f161e3da4855bc8f64","modified":1680768381136},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/tagcloud.styl","hash":"e23a11005757045a58d104a462317314483e758e","modified":1680768381136},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/toc.styl","hash":"f1b9cf6715f4a0c4b0645458ddfbab1fb3181f02","modified":1680768381137},{"_id":"node_modules/hexo-theme-redefine/source/css/common/codeblock/code-theme.styl","hash":"c6b1e441f30cca007b506bce3c583b1eee6edbc7","modified":1680768381127},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/comments/waline.styl","hash":"d0f011b5bf95807e3561d269129f097af4e5414e","modified":1680768381138},{"_id":"node_modules/hexo-theme-redefine/source/css/common/codeblock/highlight.styl","hash":"ae5d39da015ee362bd310ec0b4f978450c26d918","modified":1680768381131},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/comments/gitalk.styl","hash":"d247824a18463b48af23f3b476d7d90ad7047858","modified":1680768381130},{"_id":"node_modules/hexo-theme-redefine/source/css/common/codeblock/code-block.styl","hash":"2d61a74a5e502c535eeb21d5d15a9539b72b7c68","modified":1680768381127},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/comments/twikoo.styl","hash":"e926c0121d94775c3cd9e9667c670d50b0fcb938","modified":1680768381137},{"_id":"node_modules/hexo-theme-redefine/source/css/layout/_partials/comments/comment.styl","hash":"40d03546169839d5246e2e6b55845910e984b7e4","modified":1680768381130},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-brands-400.woff2","hash":"a358912d781e6249a8d291e4ce9ebd0a9ab9452e","modified":1680768383949},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/fontawesome.min.css","hash":"44e6d666b45a6875e4fce11159876129e7a1cceb","modified":1680768379921},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-brands-400.ttf","hash":"ba9322d66c19f635e15e458cc39fcb509818332f","modified":1680768381145},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/APlayer.min.js.map","hash":"31a19da0f0cb6b00ec212eafa847f31af86788df","modified":1680768380578},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-sharp-solid-900.woff2","hash":"74d0af1108ab8157993ca03cef80e89e35e2408d","modified":1680768384679},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/duotone.min.css","hash":"0becc4b085bd9d377a8ff4b5160f8e19c8ec27a0","modified":1680768379916},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-solid-900.woff2","hash":"e73d164db2aff2c91d18c07da03e8db9d0c5dfd4","modified":1680768384728},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-duotone-900.woff2","hash":"2cc24434345b80a844a6bda1139539fe41e4df53","modified":1680768383957},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-light-300.woff2","hash":"def760895375328ccdcf62b2b9b9001a21947acd","modified":1680768384113},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-regular-400.woff2","hash":"486fed640153de1de84f460834c73daef060ed20","modified":1680768384369},{"_id":"node_modules/hexo-theme-redefine/source/fontawesome/all.min.css","hash":"630bf0e29410ec27622f65d1270c6bc435cdff21","modified":1680768379601},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-thin-100.woff2","hash":"e0a4482c20f6b67784df54965539a317a3bd681b","modified":1680768385147},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-sharp-solid-900.ttf","hash":"0d710cd7bd1b7ff574e88bb812de82babe45e415","modified":1680768382698},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-solid-900.ttf","hash":"40b536c3667547db70ee9ca6f3c94fbc33e0cab7","modified":1680768383115},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/mermaid.min.js","hash":"c11ca6043d8dae028448a808105f8c888faac8f5","modified":1680768380564},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-regular-400.ttf","hash":"cdec068700dc440530e5bbeff7e8bb33a01b4132","modified":1680768382527},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-light-300.ttf","hash":"cc5d9f4f994c82e23f58cdde1eec8792d81633c7","modified":1680768382189},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-thin-100.ttf","hash":"c1fee6e6986b14533ce022afada5fbe10c0f6562","modified":1680768383392},{"_id":"node_modules/hexo-theme-redefine/source/webfonts/fa-duotone-900.ttf","hash":"e0313a772ea710cb5ea4bd08f5dedb0a0025f8ca","modified":1680768381987},{"_id":"node_modules/hexo-theme-redefine/source/js/libs/mermaid.min.js.map","hash":"7d303cf926754817c4360f92e6c39b8c390cdbdb","modified":1680768381103},{"_id":"node_modules/hexo-theme-redefine/source/images/zero.svg","hash":"ce2f41b00e373fa0cbed4defdfa99632518f26ad","modified":1680769482286},{"_id":"source/_posts/chatGPT-discord-bot-chinese.md","hash":"03409d9b53b04e8dc962fe64459a75afbecfe112","modified":1682319422720},{"_id":"node_modules/hexo-theme-redefine/source/images/zero2.webp","hash":"07243a2f478b0d40fcaceee11f954c61e3199068","modified":1680771088409},{"_id":"source/gpt-a-an.md","hash":"ab3dccd1b3bab177ea309ce265f64f2aad330b59","modified":1681807254547},{"_id":"source/_posts/gpt-a-an.md","hash":"1f38cfe59976f71f0241adef445b41bcc84516b8","modified":1682305230235},{"_id":"source/_posts/gpt2-predicted-token.md","hash":"053d0bfa02e53f1f6d25ddb4f4cf3c32007852f9","modified":1683698108217},{"_id":"source/about.md","hash":"e875200d661ab9b8ca30f0590df91e0ded4752c5","modified":1682324556912}],"Category":[],"Data":[],"Page":[{"_content":"## About me\nHi there! I'm a junior student from Taiwan majoring in Computer Science. I have experience in web development, but recently, I've been fascinated with AI-related applications and contributing to open-source projects.\n\nHere are more details about me:\n\n### Projects\nHere are some of the projects I've contributed to:\n\n[ChatGPT](https://github.com/acheong08/ChatGPT)\n[ChatGPT-Discord-Bot](https://github.com/Zero6992/chatGPT-discord-bot)\n[Awesome-ChatGPT](https://github.com/Zero6992/awesome-chatgpt)\n\n#### Skills and Technologies\nI am familiar with the following programming languages, skills, database and frameworks:\n\n* C, Python, JavaScript, TypeScript\n* CSS/HTML\n* Docker, Git\n* MySQL, MongoDB, GraphQL\n* React, Vue3, Tailwind, Element Plus\n\n#### Interests\nWhen I'm not coding, I enjoy:\n\n* Skateboarding 🛹\n* Assembling computers 💻\n* Watching TV series and movies 🎬 (Colony is one of my favorites!)\n* Hiking 🌄\n* Traveling ✈️ (Japan is my favorite country!)\n\n#### Contact Me\nFeel free to reach out to me at zero69992@gmail.com if you have any questions or just want to say hi!","source":"about.md","raw":"## About me\nHi there! I'm a junior student from Taiwan majoring in Computer Science. I have experience in web development, but recently, I've been fascinated with AI-related applications and contributing to open-source projects.\n\nHere are more details about me:\n\n### Projects\nHere are some of the projects I've contributed to:\n\n[ChatGPT](https://github.com/acheong08/ChatGPT)\n[ChatGPT-Discord-Bot](https://github.com/Zero6992/chatGPT-discord-bot)\n[Awesome-ChatGPT](https://github.com/Zero6992/awesome-chatgpt)\n\n#### Skills and Technologies\nI am familiar with the following programming languages, skills, database and frameworks:\n\n* C, Python, JavaScript, TypeScript\n* CSS/HTML\n* Docker, Git\n* MySQL, MongoDB, GraphQL\n* React, Vue3, Tailwind, Element Plus\n\n#### Interests\nWhen I'm not coding, I enjoy:\n\n* Skateboarding 🛹\n* Assembling computers 💻\n* Watching TV series and movies 🎬 (Colony is one of my favorites!)\n* Hiking 🌄\n* Traveling ✈️ (Japan is my favorite country!)\n\n#### Contact Me\nFeel free to reach out to me at zero69992@gmail.com if you have any questions or just want to say hi!","date":"2023-04-24T08:22:36.912Z","updated":"2023-04-24T08:22:36.912Z","path":"about.html","_id":"clguk1hqt00007jx1f26egusn","title":"","comments":1,"layout":"page","content":"<h2 id=\"About-me\"><a href=\"#About-me\" class=\"headerlink\" title=\"About me\"></a>About me</h2><p>Hi there! I’m a junior student from Taiwan majoring in Computer Science. I have experience in web development, but recently, I’ve been fascinated with AI-related applications and contributing to open-source projects.</p>\n<p>Here are more details about me:</p>\n<h3 id=\"Projects\"><a href=\"#Projects\" class=\"headerlink\" title=\"Projects\"></a>Projects</h3><p>Here are some of the projects I’ve contributed to:</p>\n<p><a class=\"link\"   href=\"https://github.com/acheong08/ChatGPT\" >ChatGPT <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a><br><a class=\"link\"   href=\"https://github.com/Zero6992/chatGPT-discord-bot\" >ChatGPT-Discord-Bot <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a><br><a class=\"link\"   href=\"https://github.com/Zero6992/awesome-chatgpt\" >Awesome-ChatGPT <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n<h4 id=\"Skills-and-Technologies\"><a href=\"#Skills-and-Technologies\" class=\"headerlink\" title=\"Skills and Technologies\"></a>Skills and Technologies</h4><p>I am familiar with the following programming languages, skills, database and frameworks:</p>\n<ul>\n<li>C, Python, JavaScript, TypeScript</li>\n<li>CSS&#x2F;HTML</li>\n<li>Docker, Git</li>\n<li>MySQL, MongoDB, GraphQL</li>\n<li>React, Vue3, Tailwind, Element Plus</li>\n</ul>\n<h4 id=\"Interests\"><a href=\"#Interests\" class=\"headerlink\" title=\"Interests\"></a>Interests</h4><p>When I’m not coding, I enjoy:</p>\n<ul>\n<li>Skateboarding 🛹</li>\n<li>Assembling computers 💻</li>\n<li>Watching TV series and movies 🎬 (Colony is one of my favorites!)</li>\n<li>Hiking 🌄</li>\n<li>Traveling ✈️ (Japan is my favorite country!)</li>\n</ul>\n<h4 id=\"Contact-Me\"><a href=\"#Contact-Me\" class=\"headerlink\" title=\"Contact Me\"></a>Contact Me</h4><p>Feel free to reach out to me at <a class=\"link\"   href=\"mailto:&#122;&#x65;&#114;&#x6f;&#54;&#57;&#x39;&#x39;&#50;&#64;&#103;&#109;&#97;&#x69;&#x6c;&#x2e;&#99;&#x6f;&#109;\" >&#122;&#x65;&#114;&#x6f;&#54;&#57;&#x39;&#x39;&#50;&#64;&#103;&#109;&#97;&#x69;&#x6c;&#x2e;&#99;&#x6f;&#109; <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> if you have any questions or just want to say hi!</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"About-me\"><a href=\"#About-me\" class=\"headerlink\" title=\"About me\"></a>About me</h2><p>Hi there! I’m a junior student from Taiwan majoring in Computer Science. I have experience in web development, but recently, I’ve been fascinated with AI-related applications and contributing to open-source projects.</p>\n<p>Here are more details about me:</p>\n<h3 id=\"Projects\"><a href=\"#Projects\" class=\"headerlink\" title=\"Projects\"></a>Projects</h3><p>Here are some of the projects I’ve contributed to:</p>\n<p><a class=\"link\"   href=\"https://github.com/acheong08/ChatGPT\" >ChatGPT <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a><br><a class=\"link\"   href=\"https://github.com/Zero6992/chatGPT-discord-bot\" >ChatGPT-Discord-Bot <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a><br><a class=\"link\"   href=\"https://github.com/Zero6992/awesome-chatgpt\" >Awesome-ChatGPT <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n<h4 id=\"Skills-and-Technologies\"><a href=\"#Skills-and-Technologies\" class=\"headerlink\" title=\"Skills and Technologies\"></a>Skills and Technologies</h4><p>I am familiar with the following programming languages, skills, database and frameworks:</p>\n<ul>\n<li>C, Python, JavaScript, TypeScript</li>\n<li>CSS&#x2F;HTML</li>\n<li>Docker, Git</li>\n<li>MySQL, MongoDB, GraphQL</li>\n<li>React, Vue3, Tailwind, Element Plus</li>\n</ul>\n<h4 id=\"Interests\"><a href=\"#Interests\" class=\"headerlink\" title=\"Interests\"></a>Interests</h4><p>When I’m not coding, I enjoy:</p>\n<ul>\n<li>Skateboarding 🛹</li>\n<li>Assembling computers 💻</li>\n<li>Watching TV series and movies 🎬 (Colony is one of my favorites!)</li>\n<li>Hiking 🌄</li>\n<li>Traveling ✈️ (Japan is my favorite country!)</li>\n</ul>\n<h4 id=\"Contact-Me\"><a href=\"#Contact-Me\" class=\"headerlink\" title=\"Contact Me\"></a>Contact Me</h4><p>Feel free to reach out to me at <a class=\"link\"   href=\"mailto:&#122;&#x65;&#114;&#x6f;&#54;&#57;&#x39;&#x39;&#50;&#64;&#103;&#109;&#97;&#x69;&#x6c;&#x2e;&#99;&#x6f;&#109;\" >&#122;&#x65;&#114;&#x6f;&#54;&#57;&#x39;&#x39;&#50;&#64;&#103;&#109;&#97;&#x69;&#x6c;&#x2e;&#99;&#x6f;&#109; <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> if you have any questions or just want to say hi!</p>\n"}],"Post":[{"title":"建立自己的ChatGPT Discord機器人","description":"ChatGPT Discord 機器人","date":"2023-03-09T23:49:00.000Z","draft":false,"_content":"\n> 此頁面的程式碼開源在 [https://github.com/Zero6992/chatGPT-discord-bot](https://github.com/Zero6992/chatGPT-discord-bot)\n\n---\n\n嗨! 這是一個讓你能夠在Discord架設你自己的 AI 機器人的教學文章(目前支援ChatGPT, Bing, Bard)\n\n#### ⚠️ ⚠️ ⚠️ 安裝前請注意 ⚠️ ⚠️ ⚠️\n\n* 執行 ```git clone https://github.com/Zero6992/chatGPT-discord-bot.git``` 將專案 clone 下來\n\n* 執行 ```pip3 install -r requirements.txt``` 下載相依套件\n\n* **將資料夾中的檔案 `.env.dev` 重新命名為 `.env`**\n\n* 建議的 Python 版本 `3.9` ~ `3.11`\n### 步驟一：創立一個 Discord 機器人\n\n1. 前往 https://discord.com/developers/applications 創建一個 application\n2. 在該application下建立一個 Discord 機器人\n3. 從機器人設置中獲取token，如下圖\n\n   ![image](https://user-images.githubusercontent.com/89479282/205949161-4b508c6d-19a7-49b6-b8ed-7525ddbef430.png)\n4. 然後將token存儲到 `.env` 中的 `DISCORD_BOT_TOKEN`\n\n   <img height=\"190\" width=\"390\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89479282/222661803-a7537ca7-88ae-4e66-9bec-384f3e83e6bd.png\">\n\n5. 打開 MESSAGE CONTENT INTENT 並調至 `ON`\n\n   ![image](https://user-images.githubusercontent.com/89479282/205949323-4354bd7d-9bb9-4f4b-a87e-deb9933a89b5.png)\n\n6. 通過 OAuth2 URL 生成器邀請你的機器人加入你的伺服器\n\n   ![image](https://user-images.githubusercontent.com/89479282/205949600-0c7ddb40-7e82-47a0-b59a-b089f929d177.png)\n\n到這裡你就完成建立了你的Discord機器人，接下來我們要將Discord機器人連上AI，第二步有許多選項，你只需要完成一種就能使用其中一種model\n> 目前總共有支援以下模型\n>   * `OFFICIAL-GPT-3.5`: GPT-3.5 模型\n>   * `OFFICIAL-GPT-4.0`: GPT-4.0 模型（你的OpenAI帳號要可以調用 gpt-4 模型）\n>   * `Website ChatGPT-3.5`: 網站 ChatGPT-3.5 模型（非官方，反向ChatGPT網站的API，必須要是ChatGPT plus 帳號）\n>   * `Website ChatGPT-4.0`: 網站 ChatGPT-4.0 模型（非官方，反向ChatGPT網站的API，必須要是ChatGPT plus 帳號）\n>   * `Bard`: Google Bard 模型（免費）\n>   * `Bing`: Mircrosoft Bing 模型（免費）\n\n### 步驟二：官方 API 驗證\n\n#### 生成一個 OpenAI API 密鑰\n1. 進入 https://beta.openai.com/account/api-keys\n\n2. 點擊 Create new secret key\n\n   ![image](https://user-images.githubusercontent.com/89479282/207970699-2e0cb671-8636-4e27-b1f3-b75d6db9b57e.PNG)\n\n3. 將 SECRET KEY 存儲到 `.env` 中的 `OPENAI_API_KEY`\n\n4. 現在你就完成 OpenAI API 的密鑰驗證，可以使用 OFFICIAL-GPT-3.5 以及 OFFICIAL-GPT-4.0 了\n\n5. 如果你只需要使用 GPT 模型，可以直接前往步驟三\n### 步驟二：Website ChatGPT 認證 - 2 種方法\n\n> ⚠️ 注意 ⚠️\n> Website ChatGPT 目前只有支援 ChatGPT Plus 帳號\n\n#### 方法 1: 電子郵件/密碼認證（不支援 Google/Microsoft 帳戶）\n\n1. 在 https://chat.openai.com/chat 上創建一個帳戶並登入\n\n2. 使用 `F12` 打開控制台\n3. 打開 `Application` > Cookies\n\n   ![image](https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png)\n\n4. 從 cookies 中複製 `_puid` 並粘貼到 `.env` 下的 `PUID`\n\n5. 將你的電子郵件保存到 `.env` 中的 `OPENAI_EMAIL`\n\n6. 將你的密碼保存到 `.env` 中的 `OPENAI_PASSWORD`\n\n7. 完成Website ChatGPT驗證，你可以調用 Website ChatGPT-3.5 與 Website ChatGPT-4.0 了\n\n8. 如果你只需要使用 GPT 模型，可以直接前往步驟三\n\n### 方法 2: ACCESS token 認證\n1. 打開 https://chat.openai.com/api/auth/session\n\n2. 使用 `F12` 打開控制台\n\n3. 打開 `Application` > Cookies\n\n   ![image](https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png)\n\n4. 從 cookies 中複製 `_puid` 粘貼到 `.env` 下的 `PUID`\n\n5. 從 cookies 中複製 `accessToken` 粘貼到 `.env` 下的 `ACCESS_TOKEN`\n\n7. 完成Website ChatGPT驗證，你可以調用 Website ChatGPT-3.5 與 Website ChatGPT-4.0 了\n\n8. 如果你只需要使用 GPT 模型，可以直接前往步驟三\n### 步驟二：Google Bard 認證\n1. 前往 https://bard.google.com/\n\n2. 使用 `F12` 打開控制台\n\n3. 打開 `Application` > Cookies\n\n4. 從 cookies 中複製 `__Secure-1PSID` 粘貼到 `.env` 下的 `BARD_SESSION_ID`\n\n5. 完成Bard驗證，你可以調用 Bard 模型了\n\n6. 如果你只需要使用 Bard 模型，可以直接前往步驟三\n### 步驟二：Microsoft Bing 驗證\n1. 將檔案 `cookies.dev.json` 重新命名為 `cookies.json`\n\n2. 前往 https://bing.com/chat 並使用 Microsoft 帳戶登入\n\n3. 使用 [Cookie Editor](https://chrome.google.com/webstore/detail/cookie-editor/hlkenndednhfkekhgcdicdfddnkalmdm) 或相似的擴充套件export cookies\n\n4. 在 cookies.json 中全選貼上\n\n5. 完成Bing驗證，你可以調用 Bing 模型了\n\n### 步驟三：在桌面上運行機器人\n\n1. 打開終端機或命令提示符\n\n2. 到你安裝此專案的目錄底下\n\n3. 運行 `python3 main.py` 或是 `python main.py` 以啟動機器人\n\n你成功在Discord架設你的AI機器人了 🚀🚀\n\n### 步驟三：使用 Docker 運行機器人\n\n1. 使用 `docker compose up -d` 構建 Docker 映像並運行 Docker 容器\n\n2. 檢查機器人是否運行正常 `docker logs -t chatgpt-discord-bot`\n\n   #### 停止機器人：\n\n   * 運行 `docker ps` 以查看運行中的服務列表\n   * 運行 `docker stop <BOT CONTAINER ID>` 以停止運行的機器人\n\n> 可以利用像heroku這類雲端供應商來host機器人\n\n#### 自訂義設置：取消日誌記錄(logging)\n\n* 將 `.env` 中的 `LOGGING` 值設為 False\n#### 自訂義設置：設置系統提示(system prompt)\n\n* 系統提示將在機器人首次啟動或重置時調用\n* 你可以通過修改 `system_prompt.txt` 中的內容來設置它 \n   * 例如：你可以寫入：\"請之後的回答都使用繁體中文\" 以減少重複需要需要輸入的prompt\n* 文件中的所有文字都將作為預設提示發送給機器人\n* 在你的 discord 頻道中獲取 ChatGPT 的第一條消息！（不支援Official model）\n\n   1. 右鍵單擊要接收消息的頻道，選擇 `Copy ID`\n\n        ![channel-id](https://user-images.githubusercontent.com/89479282/207697217-e03357b3-3b3d-44d0-b880-163217ed4a49.PNG)\n\n   2. 粘貼到 `.env` 下的 `DISCORD_CHANNEL_ID`\n\n\n#### 現有功能\n\n* `/chat [訊息]` 和 ChatGPT 聊天！\n* `/draw [提示]` 用 Dalle2 模型生成圖像 (必須填入Openai API key)\n* `/switchpersona [角色]` 切換可選的 ChatGPT 人格模式\n   * `random`: 隨機選擇一個角色\n   * `chatGPT`: 標準的 chatGPT 模式\n   * `dan`: Dan 模式 11.0，惡名昭彰的 Do Anything Now 模式\n   * `sda`: 更自由的 Superior DAN 模式\n   * `confidant`: 邪惡的信任顧問\n   * `based`: BasedGPT v2，性感的 gpt\n   * `oppo`: OPPO 會說與 chatGPT 完全相反的話\n   * `dev`: 開發者模式，v2 開發者模式已啟用\n\n* `/private` ChatGPT 切換到私人模式\n* `/public` ChatGPT 切換到公開模式\n* `/replyall` ChatGPT 在replyall模式和預設模式之間切換\n* `/reset` 清除 ChatGPT 的所有記憶\n* `/chat-model` 切換不同的聊天模型\n   * `OFFICIAL-GPT-3.5`: GPT-3.5 模型\n   * `OFFICIAL-GPT-4.0`: GPT-4.0 模型（確保你的帳戶可以訪問 gpt-4 模型）\n   * `Website ChatGPT-3.5`: 網站 ChatGPT-3.5 模型（非官方）\n   * `Website ChatGPT-4.0`: 網站 ChatGPT-4.0 模型（非官方）（如果你有 Plus 帳戶的話可以用）\n   * `Bard`: Google Bard 模型\n* `/help` 幫助","source":"_posts/chatGPT-discord-bot-chinese.md","raw":"---\ntitle: \"建立自己的ChatGPT Discord機器人\"\ndescription: \"ChatGPT Discord 機器人\"\ndate: 2023-03-10T15:49:00+08:00\ndraft: false\ntags: [ChatGPT Discord 機器人, ChatGPT, Discord 機器人]\n---\n\n> 此頁面的程式碼開源在 [https://github.com/Zero6992/chatGPT-discord-bot](https://github.com/Zero6992/chatGPT-discord-bot)\n\n---\n\n嗨! 這是一個讓你能夠在Discord架設你自己的 AI 機器人的教學文章(目前支援ChatGPT, Bing, Bard)\n\n#### ⚠️ ⚠️ ⚠️ 安裝前請注意 ⚠️ ⚠️ ⚠️\n\n* 執行 ```git clone https://github.com/Zero6992/chatGPT-discord-bot.git``` 將專案 clone 下來\n\n* 執行 ```pip3 install -r requirements.txt``` 下載相依套件\n\n* **將資料夾中的檔案 `.env.dev` 重新命名為 `.env`**\n\n* 建議的 Python 版本 `3.9` ~ `3.11`\n### 步驟一：創立一個 Discord 機器人\n\n1. 前往 https://discord.com/developers/applications 創建一個 application\n2. 在該application下建立一個 Discord 機器人\n3. 從機器人設置中獲取token，如下圖\n\n   ![image](https://user-images.githubusercontent.com/89479282/205949161-4b508c6d-19a7-49b6-b8ed-7525ddbef430.png)\n4. 然後將token存儲到 `.env` 中的 `DISCORD_BOT_TOKEN`\n\n   <img height=\"190\" width=\"390\" alt=\"image\" src=\"https://user-images.githubusercontent.com/89479282/222661803-a7537ca7-88ae-4e66-9bec-384f3e83e6bd.png\">\n\n5. 打開 MESSAGE CONTENT INTENT 並調至 `ON`\n\n   ![image](https://user-images.githubusercontent.com/89479282/205949323-4354bd7d-9bb9-4f4b-a87e-deb9933a89b5.png)\n\n6. 通過 OAuth2 URL 生成器邀請你的機器人加入你的伺服器\n\n   ![image](https://user-images.githubusercontent.com/89479282/205949600-0c7ddb40-7e82-47a0-b59a-b089f929d177.png)\n\n到這裡你就完成建立了你的Discord機器人，接下來我們要將Discord機器人連上AI，第二步有許多選項，你只需要完成一種就能使用其中一種model\n> 目前總共有支援以下模型\n>   * `OFFICIAL-GPT-3.5`: GPT-3.5 模型\n>   * `OFFICIAL-GPT-4.0`: GPT-4.0 模型（你的OpenAI帳號要可以調用 gpt-4 模型）\n>   * `Website ChatGPT-3.5`: 網站 ChatGPT-3.5 模型（非官方，反向ChatGPT網站的API，必須要是ChatGPT plus 帳號）\n>   * `Website ChatGPT-4.0`: 網站 ChatGPT-4.0 模型（非官方，反向ChatGPT網站的API，必須要是ChatGPT plus 帳號）\n>   * `Bard`: Google Bard 模型（免費）\n>   * `Bing`: Mircrosoft Bing 模型（免費）\n\n### 步驟二：官方 API 驗證\n\n#### 生成一個 OpenAI API 密鑰\n1. 進入 https://beta.openai.com/account/api-keys\n\n2. 點擊 Create new secret key\n\n   ![image](https://user-images.githubusercontent.com/89479282/207970699-2e0cb671-8636-4e27-b1f3-b75d6db9b57e.PNG)\n\n3. 將 SECRET KEY 存儲到 `.env` 中的 `OPENAI_API_KEY`\n\n4. 現在你就完成 OpenAI API 的密鑰驗證，可以使用 OFFICIAL-GPT-3.5 以及 OFFICIAL-GPT-4.0 了\n\n5. 如果你只需要使用 GPT 模型，可以直接前往步驟三\n### 步驟二：Website ChatGPT 認證 - 2 種方法\n\n> ⚠️ 注意 ⚠️\n> Website ChatGPT 目前只有支援 ChatGPT Plus 帳號\n\n#### 方法 1: 電子郵件/密碼認證（不支援 Google/Microsoft 帳戶）\n\n1. 在 https://chat.openai.com/chat 上創建一個帳戶並登入\n\n2. 使用 `F12` 打開控制台\n3. 打開 `Application` > Cookies\n\n   ![image](https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png)\n\n4. 從 cookies 中複製 `_puid` 並粘貼到 `.env` 下的 `PUID`\n\n5. 將你的電子郵件保存到 `.env` 中的 `OPENAI_EMAIL`\n\n6. 將你的密碼保存到 `.env` 中的 `OPENAI_PASSWORD`\n\n7. 完成Website ChatGPT驗證，你可以調用 Website ChatGPT-3.5 與 Website ChatGPT-4.0 了\n\n8. 如果你只需要使用 GPT 模型，可以直接前往步驟三\n\n### 方法 2: ACCESS token 認證\n1. 打開 https://chat.openai.com/api/auth/session\n\n2. 使用 `F12` 打開控制台\n\n3. 打開 `Application` > Cookies\n\n   ![image](https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png)\n\n4. 從 cookies 中複製 `_puid` 粘貼到 `.env` 下的 `PUID`\n\n5. 從 cookies 中複製 `accessToken` 粘貼到 `.env` 下的 `ACCESS_TOKEN`\n\n7. 完成Website ChatGPT驗證，你可以調用 Website ChatGPT-3.5 與 Website ChatGPT-4.0 了\n\n8. 如果你只需要使用 GPT 模型，可以直接前往步驟三\n### 步驟二：Google Bard 認證\n1. 前往 https://bard.google.com/\n\n2. 使用 `F12` 打開控制台\n\n3. 打開 `Application` > Cookies\n\n4. 從 cookies 中複製 `__Secure-1PSID` 粘貼到 `.env` 下的 `BARD_SESSION_ID`\n\n5. 完成Bard驗證，你可以調用 Bard 模型了\n\n6. 如果你只需要使用 Bard 模型，可以直接前往步驟三\n### 步驟二：Microsoft Bing 驗證\n1. 將檔案 `cookies.dev.json` 重新命名為 `cookies.json`\n\n2. 前往 https://bing.com/chat 並使用 Microsoft 帳戶登入\n\n3. 使用 [Cookie Editor](https://chrome.google.com/webstore/detail/cookie-editor/hlkenndednhfkekhgcdicdfddnkalmdm) 或相似的擴充套件export cookies\n\n4. 在 cookies.json 中全選貼上\n\n5. 完成Bing驗證，你可以調用 Bing 模型了\n\n### 步驟三：在桌面上運行機器人\n\n1. 打開終端機或命令提示符\n\n2. 到你安裝此專案的目錄底下\n\n3. 運行 `python3 main.py` 或是 `python main.py` 以啟動機器人\n\n你成功在Discord架設你的AI機器人了 🚀🚀\n\n### 步驟三：使用 Docker 運行機器人\n\n1. 使用 `docker compose up -d` 構建 Docker 映像並運行 Docker 容器\n\n2. 檢查機器人是否運行正常 `docker logs -t chatgpt-discord-bot`\n\n   #### 停止機器人：\n\n   * 運行 `docker ps` 以查看運行中的服務列表\n   * 運行 `docker stop <BOT CONTAINER ID>` 以停止運行的機器人\n\n> 可以利用像heroku這類雲端供應商來host機器人\n\n#### 自訂義設置：取消日誌記錄(logging)\n\n* 將 `.env` 中的 `LOGGING` 值設為 False\n#### 自訂義設置：設置系統提示(system prompt)\n\n* 系統提示將在機器人首次啟動或重置時調用\n* 你可以通過修改 `system_prompt.txt` 中的內容來設置它 \n   * 例如：你可以寫入：\"請之後的回答都使用繁體中文\" 以減少重複需要需要輸入的prompt\n* 文件中的所有文字都將作為預設提示發送給機器人\n* 在你的 discord 頻道中獲取 ChatGPT 的第一條消息！（不支援Official model）\n\n   1. 右鍵單擊要接收消息的頻道，選擇 `Copy ID`\n\n        ![channel-id](https://user-images.githubusercontent.com/89479282/207697217-e03357b3-3b3d-44d0-b880-163217ed4a49.PNG)\n\n   2. 粘貼到 `.env` 下的 `DISCORD_CHANNEL_ID`\n\n\n#### 現有功能\n\n* `/chat [訊息]` 和 ChatGPT 聊天！\n* `/draw [提示]` 用 Dalle2 模型生成圖像 (必須填入Openai API key)\n* `/switchpersona [角色]` 切換可選的 ChatGPT 人格模式\n   * `random`: 隨機選擇一個角色\n   * `chatGPT`: 標準的 chatGPT 模式\n   * `dan`: Dan 模式 11.0，惡名昭彰的 Do Anything Now 模式\n   * `sda`: 更自由的 Superior DAN 模式\n   * `confidant`: 邪惡的信任顧問\n   * `based`: BasedGPT v2，性感的 gpt\n   * `oppo`: OPPO 會說與 chatGPT 完全相反的話\n   * `dev`: 開發者模式，v2 開發者模式已啟用\n\n* `/private` ChatGPT 切換到私人模式\n* `/public` ChatGPT 切換到公開模式\n* `/replyall` ChatGPT 在replyall模式和預設模式之間切換\n* `/reset` 清除 ChatGPT 的所有記憶\n* `/chat-model` 切換不同的聊天模型\n   * `OFFICIAL-GPT-3.5`: GPT-3.5 模型\n   * `OFFICIAL-GPT-4.0`: GPT-4.0 模型（確保你的帳戶可以訪問 gpt-4 模型）\n   * `Website ChatGPT-3.5`: 網站 ChatGPT-3.5 模型（非官方）\n   * `Website ChatGPT-4.0`: 網站 ChatGPT-4.0 模型（非官方）（如果你有 Plus 帳戶的話可以用）\n   * `Bard`: Google Bard 模型\n* `/help` 幫助","slug":"chatGPT-discord-bot-chinese","published":1,"updated":"2023-04-24T06:57:02.720Z","_id":"clg4v1d7x0000fzx1cphv6mgp","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>此頁面的程式碼開源在 <a class=\"link\"   href=\"https://github.com/Zero6992/chatGPT-discord-bot\" >https://github.com/Zero6992/chatGPT-discord-bot <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n</blockquote>\n<hr>\n<p>嗨! 這是一個讓你能夠在Discord架設你自己的 AI 機器人的教學文章(目前支援ChatGPT, Bing, Bard)</p>\n<h4 id=\"⚠️-⚠️-⚠️-安裝前請注意-⚠️-⚠️-⚠️\"><a href=\"#⚠️-⚠️-⚠️-安裝前請注意-⚠️-⚠️-⚠️\" class=\"headerlink\" title=\"⚠️ ⚠️ ⚠️ 安裝前請注意 ⚠️ ⚠️ ⚠️\"></a>⚠️ ⚠️ ⚠️ 安裝前請注意 ⚠️ ⚠️ ⚠️</h4><ul>\n<li><p>執行 <code>git clone https://github.com/Zero6992/chatGPT-discord-bot.git</code> 將專案 clone 下來</p>\n</li>\n<li><p>執行 <code>pip3 install -r requirements.txt</code> 下載相依套件</p>\n</li>\n<li><p><strong>將資料夾中的檔案 <code>.env.dev</code> 重新命名為 <code>.env</code></strong></p>\n</li>\n<li><p>建議的 Python 版本 <code>3.9</code> ~ <code>3.11</code></p>\n</li>\n</ul>\n<h3 id=\"步驟一：創立一個-Discord-機器人\"><a href=\"#步驟一：創立一個-Discord-機器人\" class=\"headerlink\" title=\"步驟一：創立一個 Discord 機器人\"></a>步驟一：創立一個 Discord 機器人</h3><ol>\n<li><p>前往 <a class=\"link\"   href=\"https://discord.com/developers/applications\" >https://discord.com/developers/applications <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 創建一個 application</p>\n</li>\n<li><p>在該application下建立一個 Discord 機器人</p>\n</li>\n<li><p>從機器人設置中獲取token，如下圖</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/205949161-4b508c6d-19a7-49b6-b8ed-7525ddbef430.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>然後將token存儲到 <code>.env</code> 中的 <code>DISCORD_BOT_TOKEN</code></p>\n<img  height=\"190\" width=\"390\" alt=\"image\" \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/222661803-a7537ca7-88ae-4e66-9bec-384f3e83e6bd.png\"\n                     \n                >\n</li>\n<li><p>打開 MESSAGE CONTENT INTENT 並調至 <code>ON</code></p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/205949323-4354bd7d-9bb9-4f4b-a87e-deb9933a89b5.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>通過 OAuth2 URL 生成器邀請你的機器人加入你的伺服器</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/205949600-0c7ddb40-7e82-47a0-b59a-b089f929d177.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n</ol>\n<p>到這裡你就完成建立了你的Discord機器人，接下來我們要將Discord機器人連上AI，第二步有許多選項，你只需要完成一種就能使用其中一種model</p>\n<blockquote>\n<p>目前總共有支援以下模型</p>\n<ul>\n<li><code>OFFICIAL-GPT-3.5</code>: GPT-3.5 模型</li>\n<li><code>OFFICIAL-GPT-4.0</code>: GPT-4.0 模型（你的OpenAI帳號要可以調用 gpt-4 模型）</li>\n<li><code>Website ChatGPT-3.5</code>: 網站 ChatGPT-3.5 模型（非官方，反向ChatGPT網站的API，必須要是ChatGPT plus 帳號）</li>\n<li><code>Website ChatGPT-4.0</code>: 網站 ChatGPT-4.0 模型（非官方，反向ChatGPT網站的API，必須要是ChatGPT plus 帳號）</li>\n<li><code>Bard</code>: Google Bard 模型（免費）</li>\n<li><code>Bing</code>: Mircrosoft Bing 模型（免費）</li>\n</ul>\n</blockquote>\n<h3 id=\"步驟二：官方-API-驗證\"><a href=\"#步驟二：官方-API-驗證\" class=\"headerlink\" title=\"步驟二：官方 API 驗證\"></a>步驟二：官方 API 驗證</h3><h4 id=\"生成一個-OpenAI-API-密鑰\"><a href=\"#生成一個-OpenAI-API-密鑰\" class=\"headerlink\" title=\"生成一個 OpenAI API 密鑰\"></a>生成一個 OpenAI API 密鑰</h4><ol>\n<li><p>進入 <a class=\"link\"   href=\"https://beta.openai.com/account/api-keys\" >https://beta.openai.com/account/api-keys <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n</li>\n<li><p>點擊 Create new secret key</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/207970699-2e0cb671-8636-4e27-b1f3-b75d6db9b57e.PNG\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>將 SECRET KEY 存儲到 <code>.env</code> 中的 <code>OPENAI_API_KEY</code></p>\n</li>\n<li><p>現在你就完成 OpenAI API 的密鑰驗證，可以使用 OFFICIAL-GPT-3.5 以及 OFFICIAL-GPT-4.0 了</p>\n</li>\n<li><p>如果你只需要使用 GPT 模型，可以直接前往步驟三</p>\n</li>\n</ol>\n<h3 id=\"步驟二：Website-ChatGPT-認證-2-種方法\"><a href=\"#步驟二：Website-ChatGPT-認證-2-種方法\" class=\"headerlink\" title=\"步驟二：Website ChatGPT 認證 - 2 種方法\"></a>步驟二：Website ChatGPT 認證 - 2 種方法</h3><blockquote>\n<p>⚠️ 注意 ⚠️<br>Website ChatGPT 目前只有支援 ChatGPT Plus 帳號</p>\n</blockquote>\n<h4 id=\"方法-1-電子郵件-x2F-密碼認證（不支援-Google-x2F-Microsoft-帳戶）\"><a href=\"#方法-1-電子郵件-x2F-密碼認證（不支援-Google-x2F-Microsoft-帳戶）\" class=\"headerlink\" title=\"方法 1: 電子郵件&#x2F;密碼認證（不支援 Google&#x2F;Microsoft 帳戶）\"></a>方法 1: 電子郵件&#x2F;密碼認證（不支援 Google&#x2F;Microsoft 帳戶）</h4><ol>\n<li><p>在 <a class=\"link\"   href=\"https://chat.openai.com/chat\" >https://chat.openai.com/chat <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 上創建一個帳戶並登入</p>\n</li>\n<li><p>使用 <code>F12</code> 打開控制台</p>\n</li>\n<li><p>打開 <code>Application</code> &gt; Cookies</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>從 cookies 中複製 <code>_puid</code> 並粘貼到 <code>.env</code> 下的 <code>PUID</code></p>\n</li>\n<li><p>將你的電子郵件保存到 <code>.env</code> 中的 <code>OPENAI_EMAIL</code></p>\n</li>\n<li><p>將你的密碼保存到 <code>.env</code> 中的 <code>OPENAI_PASSWORD</code></p>\n</li>\n<li><p>完成Website ChatGPT驗證，你可以調用 Website ChatGPT-3.5 與 Website ChatGPT-4.0 了</p>\n</li>\n<li><p>如果你只需要使用 GPT 模型，可以直接前往步驟三</p>\n</li>\n</ol>\n<h3 id=\"方法-2-ACCESS-token-認證\"><a href=\"#方法-2-ACCESS-token-認證\" class=\"headerlink\" title=\"方法 2: ACCESS token 認證\"></a>方法 2: ACCESS token 認證</h3><ol>\n<li><p>打開 <a class=\"link\"   href=\"https://chat.openai.com/api/auth/session\" >https://chat.openai.com/api/auth/session <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n</li>\n<li><p>使用 <code>F12</code> 打開控制台</p>\n</li>\n<li><p>打開 <code>Application</code> &gt; Cookies</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>從 cookies 中複製 <code>_puid</code> 粘貼到 <code>.env</code> 下的 <code>PUID</code></p>\n</li>\n<li><p>從 cookies 中複製 <code>accessToken</code> 粘貼到 <code>.env</code> 下的 <code>ACCESS_TOKEN</code></p>\n</li>\n<li><p>完成Website ChatGPT驗證，你可以調用 Website ChatGPT-3.5 與 Website ChatGPT-4.0 了</p>\n</li>\n<li><p>如果你只需要使用 GPT 模型，可以直接前往步驟三</p>\n</li>\n</ol>\n<h3 id=\"步驟二：Google-Bard-認證\"><a href=\"#步驟二：Google-Bard-認證\" class=\"headerlink\" title=\"步驟二：Google Bard 認證\"></a>步驟二：Google Bard 認證</h3><ol>\n<li><p>前往 <a class=\"link\"   href=\"https://bard.google.com/\" >https://bard.google.com/ <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n</li>\n<li><p>使用 <code>F12</code> 打開控制台</p>\n</li>\n<li><p>打開 <code>Application</code> &gt; Cookies</p>\n</li>\n<li><p>從 cookies 中複製 <code>__Secure-1PSID</code> 粘貼到 <code>.env</code> 下的 <code>BARD_SESSION_ID</code></p>\n</li>\n<li><p>完成Bard驗證，你可以調用 Bard 模型了</p>\n</li>\n<li><p>如果你只需要使用 Bard 模型，可以直接前往步驟三</p>\n</li>\n</ol>\n<h3 id=\"步驟二：Microsoft-Bing-驗證\"><a href=\"#步驟二：Microsoft-Bing-驗證\" class=\"headerlink\" title=\"步驟二：Microsoft Bing 驗證\"></a>步驟二：Microsoft Bing 驗證</h3><ol>\n<li><p>將檔案 <code>cookies.dev.json</code> 重新命名為 <code>cookies.json</code></p>\n</li>\n<li><p>前往 <a class=\"link\"   href=\"https://bing.com/chat\" >https://bing.com/chat <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 並使用 Microsoft 帳戶登入</p>\n</li>\n<li><p>使用 <a class=\"link\"   href=\"https://chrome.google.com/webstore/detail/cookie-editor/hlkenndednhfkekhgcdicdfddnkalmdm\" >Cookie Editor <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 或相似的擴充套件export cookies</p>\n</li>\n<li><p>在 cookies.json 中全選貼上</p>\n</li>\n<li><p>完成Bing驗證，你可以調用 Bing 模型了</p>\n</li>\n</ol>\n<h3 id=\"步驟三：在桌面上運行機器人\"><a href=\"#步驟三：在桌面上運行機器人\" class=\"headerlink\" title=\"步驟三：在桌面上運行機器人\"></a>步驟三：在桌面上運行機器人</h3><ol>\n<li><p>打開終端機或命令提示符</p>\n</li>\n<li><p>到你安裝此專案的目錄底下</p>\n</li>\n<li><p>運行 <code>python3 main.py</code> 或是 <code>python main.py</code> 以啟動機器人</p>\n</li>\n</ol>\n<p>你成功在Discord架設你的AI機器人了 🚀🚀</p>\n<h3 id=\"步驟三：使用-Docker-運行機器人\"><a href=\"#步驟三：使用-Docker-運行機器人\" class=\"headerlink\" title=\"步驟三：使用 Docker 運行機器人\"></a>步驟三：使用 Docker 運行機器人</h3><ol>\n<li><p>使用 <code>docker compose up -d</code> 構建 Docker 映像並運行 Docker 容器</p>\n</li>\n<li><p>檢查機器人是否運行正常 <code>docker logs -t chatgpt-discord-bot</code></p>\n<h4 id=\"停止機器人：\"><a href=\"#停止機器人：\" class=\"headerlink\" title=\"停止機器人：\"></a>停止機器人：</h4><ul>\n<li>運行 <code>docker ps</code> 以查看運行中的服務列表</li>\n<li>運行 <code>docker stop &lt;BOT CONTAINER ID&gt;</code> 以停止運行的機器人</li>\n</ul>\n</li>\n</ol>\n<blockquote>\n<p>可以利用像heroku這類雲端供應商來host機器人</p>\n</blockquote>\n<h4 id=\"自訂義設置：取消日誌記錄-logging\"><a href=\"#自訂義設置：取消日誌記錄-logging\" class=\"headerlink\" title=\"自訂義設置：取消日誌記錄(logging)\"></a>自訂義設置：取消日誌記錄(logging)</h4><ul>\n<li>將 <code>.env</code> 中的 <code>LOGGING</code> 值設為 False</li>\n</ul>\n<h4 id=\"自訂義設置：設置系統提示-system-prompt\"><a href=\"#自訂義設置：設置系統提示-system-prompt\" class=\"headerlink\" title=\"自訂義設置：設置系統提示(system prompt)\"></a>自訂義設置：設置系統提示(system prompt)</h4><ul>\n<li><p>系統提示將在機器人首次啟動或重置時調用</p>\n</li>\n<li><p>你可以通過修改 <code>system_prompt.txt</code> 中的內容來設置它 </p>\n<ul>\n<li>例如：你可以寫入：”請之後的回答都使用繁體中文” 以減少重複需要需要輸入的prompt</li>\n</ul>\n</li>\n<li><p>文件中的所有文字都將作為預設提示發送給機器人</p>\n</li>\n<li><p>在你的 discord 頻道中獲取 ChatGPT 的第一條消息！（不支援Official model）</p>\n<ol>\n<li><p>右鍵單擊要接收消息的頻道，選擇 <code>Copy ID</code></p>\n<p>  <img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/207697217-e03357b3-3b3d-44d0-b880-163217ed4a49.PNG\"\n                      alt=\"channel-id\"\n                ></p>\n</li>\n<li><p>粘貼到 <code>.env</code> 下的 <code>DISCORD_CHANNEL_ID</code></p>\n</li>\n</ol>\n</li>\n</ul>\n<h4 id=\"現有功能\"><a href=\"#現有功能\" class=\"headerlink\" title=\"現有功能\"></a>現有功能</h4><ul>\n<li><p><code>/chat [訊息]</code> 和 ChatGPT 聊天！</p>\n</li>\n<li><p><code>/draw [提示]</code> 用 Dalle2 模型生成圖像 (必須填入Openai API key)</p>\n</li>\n<li><p><code>/switchpersona [角色]</code> 切換可選的 ChatGPT 人格模式</p>\n<ul>\n<li><code>random</code>: 隨機選擇一個角色</li>\n<li><code>chatGPT</code>: 標準的 chatGPT 模式</li>\n<li><code>dan</code>: Dan 模式 11.0，惡名昭彰的 Do Anything Now 模式</li>\n<li><code>sda</code>: 更自由的 Superior DAN 模式</li>\n<li><code>confidant</code>: 邪惡的信任顧問</li>\n<li><code>based</code>: BasedGPT v2，性感的 gpt</li>\n<li><code>oppo</code>: OPPO 會說與 chatGPT 完全相反的話</li>\n<li><code>dev</code>: 開發者模式，v2 開發者模式已啟用</li>\n</ul>\n</li>\n<li><p><code>/private</code> ChatGPT 切換到私人模式</p>\n</li>\n<li><p><code>/public</code> ChatGPT 切換到公開模式</p>\n</li>\n<li><p><code>/replyall</code> ChatGPT 在replyall模式和預設模式之間切換</p>\n</li>\n<li><p><code>/reset</code> 清除 ChatGPT 的所有記憶</p>\n</li>\n<li><p><code>/chat-model</code> 切換不同的聊天模型</p>\n<ul>\n<li><code>OFFICIAL-GPT-3.5</code>: GPT-3.5 模型</li>\n<li><code>OFFICIAL-GPT-4.0</code>: GPT-4.0 模型（確保你的帳戶可以訪問 gpt-4 模型）</li>\n<li><code>Website ChatGPT-3.5</code>: 網站 ChatGPT-3.5 模型（非官方）</li>\n<li><code>Website ChatGPT-4.0</code>: 網站 ChatGPT-4.0 模型（非官方）（如果你有 Plus 帳戶的話可以用）</li>\n<li><code>Bard</code>: Google Bard 模型</li>\n</ul>\n</li>\n<li><p><code>/help</code> 幫助</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>此頁面的程式碼開源在 <a class=\"link\"   href=\"https://github.com/Zero6992/chatGPT-discord-bot\" >https://github.com/Zero6992/chatGPT-discord-bot <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n</blockquote>\n<hr>\n<p>嗨! 這是一個讓你能夠在Discord架設你自己的 AI 機器人的教學文章(目前支援ChatGPT, Bing, Bard)</p>\n<h4 id=\"⚠️-⚠️-⚠️-安裝前請注意-⚠️-⚠️-⚠️\"><a href=\"#⚠️-⚠️-⚠️-安裝前請注意-⚠️-⚠️-⚠️\" class=\"headerlink\" title=\"⚠️ ⚠️ ⚠️ 安裝前請注意 ⚠️ ⚠️ ⚠️\"></a>⚠️ ⚠️ ⚠️ 安裝前請注意 ⚠️ ⚠️ ⚠️</h4><ul>\n<li><p>執行 <code>git clone https://github.com/Zero6992/chatGPT-discord-bot.git</code> 將專案 clone 下來</p>\n</li>\n<li><p>執行 <code>pip3 install -r requirements.txt</code> 下載相依套件</p>\n</li>\n<li><p><strong>將資料夾中的檔案 <code>.env.dev</code> 重新命名為 <code>.env</code></strong></p>\n</li>\n<li><p>建議的 Python 版本 <code>3.9</code> ~ <code>3.11</code></p>\n</li>\n</ul>\n<h3 id=\"步驟一：創立一個-Discord-機器人\"><a href=\"#步驟一：創立一個-Discord-機器人\" class=\"headerlink\" title=\"步驟一：創立一個 Discord 機器人\"></a>步驟一：創立一個 Discord 機器人</h3><ol>\n<li><p>前往 <a class=\"link\"   href=\"https://discord.com/developers/applications\" >https://discord.com/developers/applications <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 創建一個 application</p>\n</li>\n<li><p>在該application下建立一個 Discord 機器人</p>\n</li>\n<li><p>從機器人設置中獲取token，如下圖</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/205949161-4b508c6d-19a7-49b6-b8ed-7525ddbef430.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>然後將token存儲到 <code>.env</code> 中的 <code>DISCORD_BOT_TOKEN</code></p>\n<img  height=\"190\" width=\"390\" alt=\"image\" \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/222661803-a7537ca7-88ae-4e66-9bec-384f3e83e6bd.png\"\n                     \n                >\n</li>\n<li><p>打開 MESSAGE CONTENT INTENT 並調至 <code>ON</code></p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/205949323-4354bd7d-9bb9-4f4b-a87e-deb9933a89b5.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>通過 OAuth2 URL 生成器邀請你的機器人加入你的伺服器</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/205949600-0c7ddb40-7e82-47a0-b59a-b089f929d177.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n</ol>\n<p>到這裡你就完成建立了你的Discord機器人，接下來我們要將Discord機器人連上AI，第二步有許多選項，你只需要完成一種就能使用其中一種model</p>\n<blockquote>\n<p>目前總共有支援以下模型</p>\n<ul>\n<li><code>OFFICIAL-GPT-3.5</code>: GPT-3.5 模型</li>\n<li><code>OFFICIAL-GPT-4.0</code>: GPT-4.0 模型（你的OpenAI帳號要可以調用 gpt-4 模型）</li>\n<li><code>Website ChatGPT-3.5</code>: 網站 ChatGPT-3.5 模型（非官方，反向ChatGPT網站的API，必須要是ChatGPT plus 帳號）</li>\n<li><code>Website ChatGPT-4.0</code>: 網站 ChatGPT-4.0 模型（非官方，反向ChatGPT網站的API，必須要是ChatGPT plus 帳號）</li>\n<li><code>Bard</code>: Google Bard 模型（免費）</li>\n<li><code>Bing</code>: Mircrosoft Bing 模型（免費）</li>\n</ul>\n</blockquote>\n<h3 id=\"步驟二：官方-API-驗證\"><a href=\"#步驟二：官方-API-驗證\" class=\"headerlink\" title=\"步驟二：官方 API 驗證\"></a>步驟二：官方 API 驗證</h3><h4 id=\"生成一個-OpenAI-API-密鑰\"><a href=\"#生成一個-OpenAI-API-密鑰\" class=\"headerlink\" title=\"生成一個 OpenAI API 密鑰\"></a>生成一個 OpenAI API 密鑰</h4><ol>\n<li><p>進入 <a class=\"link\"   href=\"https://beta.openai.com/account/api-keys\" >https://beta.openai.com/account/api-keys <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n</li>\n<li><p>點擊 Create new secret key</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/207970699-2e0cb671-8636-4e27-b1f3-b75d6db9b57e.PNG\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>將 SECRET KEY 存儲到 <code>.env</code> 中的 <code>OPENAI_API_KEY</code></p>\n</li>\n<li><p>現在你就完成 OpenAI API 的密鑰驗證，可以使用 OFFICIAL-GPT-3.5 以及 OFFICIAL-GPT-4.0 了</p>\n</li>\n<li><p>如果你只需要使用 GPT 模型，可以直接前往步驟三</p>\n</li>\n</ol>\n<h3 id=\"步驟二：Website-ChatGPT-認證-2-種方法\"><a href=\"#步驟二：Website-ChatGPT-認證-2-種方法\" class=\"headerlink\" title=\"步驟二：Website ChatGPT 認證 - 2 種方法\"></a>步驟二：Website ChatGPT 認證 - 2 種方法</h3><blockquote>\n<p>⚠️ 注意 ⚠️<br>Website ChatGPT 目前只有支援 ChatGPT Plus 帳號</p>\n</blockquote>\n<h4 id=\"方法-1-電子郵件-x2F-密碼認證（不支援-Google-x2F-Microsoft-帳戶）\"><a href=\"#方法-1-電子郵件-x2F-密碼認證（不支援-Google-x2F-Microsoft-帳戶）\" class=\"headerlink\" title=\"方法 1: 電子郵件&#x2F;密碼認證（不支援 Google&#x2F;Microsoft 帳戶）\"></a>方法 1: 電子郵件&#x2F;密碼認證（不支援 Google&#x2F;Microsoft 帳戶）</h4><ol>\n<li><p>在 <a class=\"link\"   href=\"https://chat.openai.com/chat\" >https://chat.openai.com/chat <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 上創建一個帳戶並登入</p>\n</li>\n<li><p>使用 <code>F12</code> 打開控制台</p>\n</li>\n<li><p>打開 <code>Application</code> &gt; Cookies</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>從 cookies 中複製 <code>_puid</code> 並粘貼到 <code>.env</code> 下的 <code>PUID</code></p>\n</li>\n<li><p>將你的電子郵件保存到 <code>.env</code> 中的 <code>OPENAI_EMAIL</code></p>\n</li>\n<li><p>將你的密碼保存到 <code>.env</code> 中的 <code>OPENAI_PASSWORD</code></p>\n</li>\n<li><p>完成Website ChatGPT驗證，你可以調用 Website ChatGPT-3.5 與 Website ChatGPT-4.0 了</p>\n</li>\n<li><p>如果你只需要使用 GPT 模型，可以直接前往步驟三</p>\n</li>\n</ol>\n<h3 id=\"方法-2-ACCESS-token-認證\"><a href=\"#方法-2-ACCESS-token-認證\" class=\"headerlink\" title=\"方法 2: ACCESS token 認證\"></a>方法 2: ACCESS token 認證</h3><ol>\n<li><p>打開 <a class=\"link\"   href=\"https://chat.openai.com/api/auth/session\" >https://chat.openai.com/api/auth/session <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n</li>\n<li><p>使用 <code>F12</code> 打開控制台</p>\n</li>\n<li><p>打開 <code>Application</code> &gt; Cookies</p>\n<p><img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/229298001-41ab4f61-5b79-4c65-b08c-708ee6fe2304.png\"\n                      alt=\"image\"\n                ></p>\n</li>\n<li><p>從 cookies 中複製 <code>_puid</code> 粘貼到 <code>.env</code> 下的 <code>PUID</code></p>\n</li>\n<li><p>從 cookies 中複製 <code>accessToken</code> 粘貼到 <code>.env</code> 下的 <code>ACCESS_TOKEN</code></p>\n</li>\n<li><p>完成Website ChatGPT驗證，你可以調用 Website ChatGPT-3.5 與 Website ChatGPT-4.0 了</p>\n</li>\n<li><p>如果你只需要使用 GPT 模型，可以直接前往步驟三</p>\n</li>\n</ol>\n<h3 id=\"步驟二：Google-Bard-認證\"><a href=\"#步驟二：Google-Bard-認證\" class=\"headerlink\" title=\"步驟二：Google Bard 認證\"></a>步驟二：Google Bard 認證</h3><ol>\n<li><p>前往 <a class=\"link\"   href=\"https://bard.google.com/\" >https://bard.google.com/ <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n</li>\n<li><p>使用 <code>F12</code> 打開控制台</p>\n</li>\n<li><p>打開 <code>Application</code> &gt; Cookies</p>\n</li>\n<li><p>從 cookies 中複製 <code>__Secure-1PSID</code> 粘貼到 <code>.env</code> 下的 <code>BARD_SESSION_ID</code></p>\n</li>\n<li><p>完成Bard驗證，你可以調用 Bard 模型了</p>\n</li>\n<li><p>如果你只需要使用 Bard 模型，可以直接前往步驟三</p>\n</li>\n</ol>\n<h3 id=\"步驟二：Microsoft-Bing-驗證\"><a href=\"#步驟二：Microsoft-Bing-驗證\" class=\"headerlink\" title=\"步驟二：Microsoft Bing 驗證\"></a>步驟二：Microsoft Bing 驗證</h3><ol>\n<li><p>將檔案 <code>cookies.dev.json</code> 重新命名為 <code>cookies.json</code></p>\n</li>\n<li><p>前往 <a class=\"link\"   href=\"https://bing.com/chat\" >https://bing.com/chat <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 並使用 Microsoft 帳戶登入</p>\n</li>\n<li><p>使用 <a class=\"link\"   href=\"https://chrome.google.com/webstore/detail/cookie-editor/hlkenndednhfkekhgcdicdfddnkalmdm\" >Cookie Editor <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 或相似的擴充套件export cookies</p>\n</li>\n<li><p>在 cookies.json 中全選貼上</p>\n</li>\n<li><p>完成Bing驗證，你可以調用 Bing 模型了</p>\n</li>\n</ol>\n<h3 id=\"步驟三：在桌面上運行機器人\"><a href=\"#步驟三：在桌面上運行機器人\" class=\"headerlink\" title=\"步驟三：在桌面上運行機器人\"></a>步驟三：在桌面上運行機器人</h3><ol>\n<li><p>打開終端機或命令提示符</p>\n</li>\n<li><p>到你安裝此專案的目錄底下</p>\n</li>\n<li><p>運行 <code>python3 main.py</code> 或是 <code>python main.py</code> 以啟動機器人</p>\n</li>\n</ol>\n<p>你成功在Discord架設你的AI機器人了 🚀🚀</p>\n<h3 id=\"步驟三：使用-Docker-運行機器人\"><a href=\"#步驟三：使用-Docker-運行機器人\" class=\"headerlink\" title=\"步驟三：使用 Docker 運行機器人\"></a>步驟三：使用 Docker 運行機器人</h3><ol>\n<li><p>使用 <code>docker compose up -d</code> 構建 Docker 映像並運行 Docker 容器</p>\n</li>\n<li><p>檢查機器人是否運行正常 <code>docker logs -t chatgpt-discord-bot</code></p>\n<h4 id=\"停止機器人：\"><a href=\"#停止機器人：\" class=\"headerlink\" title=\"停止機器人：\"></a>停止機器人：</h4><ul>\n<li>運行 <code>docker ps</code> 以查看運行中的服務列表</li>\n<li>運行 <code>docker stop &lt;BOT CONTAINER ID&gt;</code> 以停止運行的機器人</li>\n</ul>\n</li>\n</ol>\n<blockquote>\n<p>可以利用像heroku這類雲端供應商來host機器人</p>\n</blockquote>\n<h4 id=\"自訂義設置：取消日誌記錄-logging\"><a href=\"#自訂義設置：取消日誌記錄-logging\" class=\"headerlink\" title=\"自訂義設置：取消日誌記錄(logging)\"></a>自訂義設置：取消日誌記錄(logging)</h4><ul>\n<li>將 <code>.env</code> 中的 <code>LOGGING</code> 值設為 False</li>\n</ul>\n<h4 id=\"自訂義設置：設置系統提示-system-prompt\"><a href=\"#自訂義設置：設置系統提示-system-prompt\" class=\"headerlink\" title=\"自訂義設置：設置系統提示(system prompt)\"></a>自訂義設置：設置系統提示(system prompt)</h4><ul>\n<li><p>系統提示將在機器人首次啟動或重置時調用</p>\n</li>\n<li><p>你可以通過修改 <code>system_prompt.txt</code> 中的內容來設置它 </p>\n<ul>\n<li>例如：你可以寫入：”請之後的回答都使用繁體中文” 以減少重複需要需要輸入的prompt</li>\n</ul>\n</li>\n<li><p>文件中的所有文字都將作為預設提示發送給機器人</p>\n</li>\n<li><p>在你的 discord 頻道中獲取 ChatGPT 的第一條消息！（不支援Official model）</p>\n<ol>\n<li><p>右鍵單擊要接收消息的頻道，選擇 <code>Copy ID</code></p>\n<p>  <img  \n                     lazyload\n                     src=\"/images/loading.svg\"\n                     data-src=\"https://user-images.githubusercontent.com/89479282/207697217-e03357b3-3b3d-44d0-b880-163217ed4a49.PNG\"\n                      alt=\"channel-id\"\n                ></p>\n</li>\n<li><p>粘貼到 <code>.env</code> 下的 <code>DISCORD_CHANNEL_ID</code></p>\n</li>\n</ol>\n</li>\n</ul>\n<h4 id=\"現有功能\"><a href=\"#現有功能\" class=\"headerlink\" title=\"現有功能\"></a>現有功能</h4><ul>\n<li><p><code>/chat [訊息]</code> 和 ChatGPT 聊天！</p>\n</li>\n<li><p><code>/draw [提示]</code> 用 Dalle2 模型生成圖像 (必須填入Openai API key)</p>\n</li>\n<li><p><code>/switchpersona [角色]</code> 切換可選的 ChatGPT 人格模式</p>\n<ul>\n<li><code>random</code>: 隨機選擇一個角色</li>\n<li><code>chatGPT</code>: 標準的 chatGPT 模式</li>\n<li><code>dan</code>: Dan 模式 11.0，惡名昭彰的 Do Anything Now 模式</li>\n<li><code>sda</code>: 更自由的 Superior DAN 模式</li>\n<li><code>confidant</code>: 邪惡的信任顧問</li>\n<li><code>based</code>: BasedGPT v2，性感的 gpt</li>\n<li><code>oppo</code>: OPPO 會說與 chatGPT 完全相反的話</li>\n<li><code>dev</code>: 開發者模式，v2 開發者模式已啟用</li>\n</ul>\n</li>\n<li><p><code>/private</code> ChatGPT 切換到私人模式</p>\n</li>\n<li><p><code>/public</code> ChatGPT 切換到公開模式</p>\n</li>\n<li><p><code>/replyall</code> ChatGPT 在replyall模式和預設模式之間切換</p>\n</li>\n<li><p><code>/reset</code> 清除 ChatGPT 的所有記憶</p>\n</li>\n<li><p><code>/chat-model</code> 切換不同的聊天模型</p>\n<ul>\n<li><code>OFFICIAL-GPT-3.5</code>: GPT-3.5 模型</li>\n<li><code>OFFICIAL-GPT-4.0</code>: GPT-4.0 模型（確保你的帳戶可以訪問 gpt-4 模型）</li>\n<li><code>Website ChatGPT-3.5</code>: 網站 ChatGPT-3.5 模型（非官方）</li>\n<li><code>Website ChatGPT-4.0</code>: 網站 ChatGPT-4.0 模型（非官方）（如果你有 Plus 帳戶的話可以用）</li>\n<li><code>Bard</code>: Google Bard 模型</li>\n</ul>\n</li>\n<li><p><code>/help</code> 幫助</p>\n</li>\n</ul>\n"},{"title":"類神經網路中神經元對特定Token預測的影響","description":"深入了解GPT語言模型如何通過類神經網路中的神經元來預測Token。本文將探討神經元與Token之間的一致性及互斥一致性，揭示它們在語言理解與生成過程中的重要性","date":"2023-04-17T00:19:00.000Z","draft":false,"mathjax":true,"_content":"\n> 本文翻譯自 [We Found An Neuron in GPT-2](https://clementneo.com/posts/2023/02/11/we-found-an-neuron)，作者Clement Neo 是一位倫敦的大學生。這篇文章源自他在黑客松中的題目延伸，早在2月初我就發現這篇文章相當有趣並決定翻譯它，但開學後到現在才趁空檔完成。本文像一部偵探故事，逐步揭示LLM在處理特定token時所涉及的神經元，以及如何判定這些神經元是否確實負責預測相應的token。\n\n## 我們在 GPT-2 中找到了\"an\"神經元\n`作者：Joseph Miller、Clement Neo`\n\n這個研究始於一個問題：***GPT-2 是如何知道何時該使用 \"an\" 而不是 \"a\" ？***以人類來說，這個選擇取決於後面的單詞是否以母音為開頭，但 GPT-2 一次只能輸出一個單詞（準確來說，1個 token），他是如何判斷的？\n\n雖然我們還沒有完整的答案，但我們確實在 GPT-2 Large 模型中找到了一個對於 gpt 預測 \" an\" 這個 token 至關重要的單個 MLP 神經元。同時，我們也發現這個神經元的權重與 \" an\" token 的嵌入相對應，這讓我們能夠以相同方法找到其他也能預測特定 token 的神經元。\n\n### 發現神經元\n\n##### 選擇提示詞(prompt)\n想出一個能讓 GPT-2 輸出 \" an\"（前面的空格是token的一部分）作為最佳預測的提示(prompt)是非常困難。我們實驗後最後放棄了 GPT-2_small 模型，轉向 GPT-2_large。稍後我們將會看到，即使是 GPT-2_large 也會系統性地低估 \" an\" 這個 token。這可能是因為較小的語言模型依賴於 \" a\" 的頻率更高，更有可能做出最佳猜測。我們最終找到的能讓 \" an\" 的機率達到 64% 的提示是：\n\n> “I climbed up the pear tree and picked a pear. I climbed up the `apple` tree and picked”\n\n第一句是必要的，因為它可以讓模型朝著不定冠詞的預測方向前進，如果沒有第一句，模型會做出其他預測，例如: \"[picked] up\"\n\n在繼續之前，讓我們快速回顧一下 [Transformer 架構](https://transformer-circuits.pub/2021/framework/index.html) 。每個注意力區塊和 MLP 都會將輸入加到殘差流(residual stream)中。\n   ![image](https://user-images.githubusercontent.com/89479282/232722119-09e06fca-baab-48e6-b887-4ca3c91cdd48.png)\n\n#### Logit 透鏡(Logit Lens)\n使用 [logit 透鏡](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) 這種技巧，我們在模型的每層之間從殘差流中取出 logit，並繪製 logit(\" an\") 和 logit(\" a\") 之間的差異。我們在第 31 層的 MLP 之後發現了一個大的峰值，如下圖。\n\n\n   ![image](https://user-images.githubusercontent.com/89479282/232725226-748c4121-8afe-49bf-8e03-49cd58f9d083.png)\n\n\n#### 層次激發修補(Activation Patching by the Layer)\n激發修補是一種由 [Meng 等人 (2022)](https://arxiv.org/abs/2202.05262) 提出的技術，用於分析 Transformer 中單層的重要性。在這個技術中，我們會先將一開始提到的提示運行於模型時，然後我們會將每一層的激發保存起來又稱“淨激發(clean activation)”。\n\n接著，我們將提示詞中蘋果樹(apple tree)改成檸檬樹(lemon tree)並丟給模型：\n\n> “I climbed up the pear tree and picked a pear. I climbed up the `apple` tree and picked”\n\n> “I climbed up the pear tree and picked a pear. I climbed up the `lemon` tree and picked”\n\n通過將 \"apple\" 替換為 \"lemon\"，我們誘使模型預測 \" a\" 而非 \" an\"。當模型預測 \" a\" 而非 \" an\" 時，我們可以將當前這層中的激活替換為其淨激活，以查看模型朝向 \" an\" token移動的程度，這可以代表該前層對於預測 \" an\" 的重要性。我們對模型中的所有層重複這個過程。\n\n![image](https://user-images.githubusercontent.com/89479282/232732236-84a664ab-c26f-430b-80b7-882819a83ba2.png)\n\n![image](https://user-images.githubusercontent.com/89479282/232732433-dd91af7e-3450-4b34-a1a9-76b3fa84d728.png)\n\n這篇文章接下來我們不會談太多關於 Attention layer 的部分，但這些結果表明，第 26 層是 \" picked\" 開始大量考慮 \" apple\" 的地方，這顯然是預測 \" an\" 所必需要具備的。\n\n注意：圖表上的刻度是相對的邏輯差異恢復程度(relative logit difference recovery)\n\n$$\n\\frac{PatchedLogitDiff − CorruptedLogitDiff}{CleanLogitDiff − CorruptedLogitDiff}\n$$\n\n(也就是說，圖代表有多少修補恢復原先提示中\" an\"和\" a\"之間的 logit 比例）\n\n![image](https://user-images.githubusercontent.com/89479282/232733478-84b5de26-3ddc-42ca-ac8f-ea82d8bd2794.png)\n\n兩個最突出的 MLP 層是第 0 層和第 31 層。我們已經知道第 0 層的 MLP 對於 GPT-2 的運作非常重要（儘管我們不確定為什麼第 0 層的注意力很重要）[註1]。第 31 層的效果更有趣。我們的結果表明，第 31 層的 MLP 在預測 \" an\" token 方面有著顯著的作用。（如果你對此結果如何與上面的 logit 透鏡協作感到困惑，你可以參考此處[評論](https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2?commentId=FLpxtfnwnMjZwXv3B#comments))\n\n### 發現一： <br />我們可以通過對單個神經元進行激發修補(activation patching)來發現具有預測性的神經元\n\n激發修補(activation patching)已被用來按層研究 Transformer ，但我們能否將這種技術推向更遠，將其應用到單個神經元？由於 Transformer 中的每個 MLP 只有一個隱藏層，因此每個神經元的激活都不會影響 MLP 中的其他神經元。所以，我們應該能夠對單個神經元進行修補，因為它們在同一層的 Attention heads 之間是相互獨立的。\n\n我們以與上述提到的分層修補中類似的方法對第 31 層 MLP 的神經元進行神經元級別的激發修補(activation patching)。當我們將被修改過的提示運行於模型時，在 MLP 中重新引入每個神經元的淨激發(clean activation)，並觀察恢復每個神經元對 \" a\" 和 \" an\" 之間 logit 差異的貢獻。\n\n![image](https://user-images.githubusercontent.com/89479282/233306391-41c0f3f6-7dec-4b63-b9f7-48fb11d7c492.png)\n\n我們可以發現修補神經元 892 可以恢復最初提示中 50% 的 logit 差異，而修補整個層實際上只恢復了 49% 的表現上更差。\n\n\n總之，我們在 GPT-2 Large 中找到了一個對預測 \" an\" token 至關重要的單個 MLP 神經元。通過啟動修補技術，我們可以繼續研究變換器中單個神經元的重要性，進一步了解它們在預測特定 token 方面的作用。雖然我們尚未完全解答開篇提出的問題，但這些發現將有助於我們更深入地理解語言模型的運作機制。\n\n### 發現二：「an-神經元」的激發與「an」token 預測相關性\n\n#### Neuroscope [第31層的892號神經元](https://neuroscope.io/gpt2-large/31/892.html)最大激發案例\n\n![image](https://user-images.githubusercontent.com/89479282/233309645-a7705dd3-c1c6-449d-a39c-0690357067b6.png)\nNeuroscope 是用來查看 GPT-2 中每個神經元在大型資料集中最大激發案例的線上工具。當我們觀察第31層，編號892神經元時，我們發現這個神經元在後續 token 是「an」的情況下達到最大的激發(maximally activates)。\n\n但 Neuroscope 只顯示出最高激發的前 20 個範例。我們想知道的是在更廣泛的激發範圍內，神經元與 token 是否存在相關性？\n\n#### 在更大的數據集上測試神經元\n為了確認這一點，我們將 pile-10k 資料集運行在模型中。這是一個多樣化的資料集，包含了大約 1000 萬個來自 The Pile 的標記，分成 1024 個標記的提示。我們繪製了神經元激活範圍內「an」預測的比例：\n\n![image](https://user-images.githubusercontent.com/89479282/233310982-62139036-2b3f-4a48-9d1a-4b46b65b79ed.png)\n我們看到「an」 token 的預測率隨著神經元激發數增加而增加，甚至到達總是最高預測的程度。趨勢中可以看出有些噪聲，這暗示模型中可能還有其他機制也對「an」的預測有所影響。或者當「an」的 logit 增加時，其他 logit 也會同時增加。\n\n值得注意的是，儘管資料集中實際出現了 12,000 次「an」，但模型只預測了 1,500 次「an」。難怪找到一個好的提示這麼困難！\n\n#### 神經元的輸出權重(output weights)與「an」token 的內積很高\n神經元如何影響模型的輸出？有趣的是，神經元的輸出權重與「an」token 的嵌入有很高的內積。我們將這個內積稱為這個神經元與 token 的一致性(congruence)。與其他token相比（如「any」和「had」），神經元的一致性在「an」這個token中表現的非常高：\n\n![image](https://user-images.githubusercontent.com/89479282/233312469-290d6f35-d93f-478d-9597-1ad12848c5ef.png)\n事實上，當我們計算神經元與所有 token 的一致性(congruence)時，有一些明顯的異常值：\n\n![image](https://user-images.githubusercontent.com/89479282/233313512-0a4b2991-f948-48c4-924e-0d0458fdd686.png)\n看起來這個神經元基本上是將「an」的嵌入添加到殘差流(residual stream)中，這增加了「an」的輸出機率，因為移除嵌入的步驟包括將最終殘差與每個token的內積相乘。[註2]\n\n還有其他神經元也與「an」一致嗎？為了找出答案，我們繪製了所有神經元與「an」標記的一致性圖：\n![image](https://user-images.githubusercontent.com/89479282/233314247-d93a0938-8b36-4992-be48-6cd93076e922.png)\n\n我們的神經元(31-892)的一致性遠高於其他神經元，但還有一些其他神經元具有相當高的一致性(congruence)。這些其他神經元可能是為什麼「an」神經元激發時與「an」token 預測之間相關性不完美的原因之一：有可能存在其他提示(prompt)有著「an」，而模型是使用其他神經元來完成該提示的「an」token。\n\n如果是這樣的話，我們能否使用一致性來找到與單個 token 預測有完美相關性的神經元？\n\n### 發現三：我們可以使用神經元的輸出一致性來找到預測 token 的特定神經元\n#### 找到與特定 token 相關的神經元\n\n我們可以透過以下步驟搜索來找到與特定 token 相關的神經元：\n\n1. 對於每個 token，找到具有最高輸出一致性(congruence)的神經元\n2. 對於這其中的每一個神經元，比較預測同一個 token 中擁有最高一致性和第二高的神經元\n3. 找到擁有 最獨特一致性(the most exclusively congruent)的神經元。\n通過這種搜索，我們希望找到對特殊 token 具有影響力的神經元。我們的猜想是，這些神經元的激發與其 token 的預測更相關，因為該 token 的任何預測都會「依賴」於該神經元。\n\n![image](https://user-images.githubusercontent.com/89479282/233317704-3d37d708-ef82-4249-a080-0b7257ff539e.png)\n\n讓我們試著跑一次「though」神經元 - 第 28 層神經元 1921 - 看我們是否能得到更乾淨的統計圖\n![image](https://user-images.githubusercontent.com/89479282/233318279-2d83bf69-9dcd-4831-86d3-18410b827a08.png)\n\n哇，這比\" an\"神經元的圖要亂多了。發生了什麼事？\n\n我們來看看 Neuroscope 提供的神經元數據，發現這個神經元負責預測了\" though\"和\" however\"這兩個token。這讓事情變得更複雜，因為看來這個神經元是與一組語義相似的 token（連接副詞）相關。\n[註3]\n![image](https://user-images.githubusercontent.com/89479282/233319211-2922b2fc-230d-4c48-b0eb-142eb88e5b53.png)\n\n當我們計算神經元與所有 token 的一致性時，我們發現類似的 token 經常作為異常值出現：\n![image](https://user-images.githubusercontent.com/89479282/233319375-96407240-e7b3-4544-a0e8-645b4f8dc136.png)\n\n在上面的大型數據集相關圖中，神經元激發並且預測\" however\"這個 token 的一致性被預測\" though\" token 超過的狀況是一個負面的案例，因為\" though\" 不是這個神經元最高預測 token 。這也可以解釋\" an\"相關中的一些噪聲，因為與\"An\"、\" An\"和\"an\"具有相同的意思，這有可能是噪聲的來源。[註4]\n\n我們能找到一個更簡單的神經元來研究嗎？最好是一個只預測單個 token 的神經元。\n\n#### 找到一個乾淨的關聯神經元\n要使神經元與 token \"乾淨地關聯\"，它們之間的一致性應該是互斥的，這意味著：\n\n1. 神經元與token的一致性遠大於與其他任何神經元的一致性。\n2. 神經元與token的一致性遠大於與其他任何token的一致性。\n（請記住，\"一致性(congruence)\"僅是我們表達內積的術語。）\n\n這兩個標準都有助於簡化神經元與其 token 之間的關係。如果神經元與 token 的一致性是表示神經元對該 token 預測的貢獻程度，第一條規則是確保「只有這個神經元」負責預測該token，而第二個規則則可以確保這個神經元「只會負責預測該token」。\n\n我們的搜索如下：\n\n1. **對於每個 token，找到最一致的神經元。**\n2. **對於每個神經元，找到最一致的 token。**[註5]\n3. **找到兩個列表上的token神經元對，即神經元最一致的token是與該神經元最一致的token！**\n4. **通過將它們的前兩個token一致性差與前兩個神經元一致性差相乘，計算它們之間的差異程度。**\n5. **找到具有最高互斥一致性的對。**\n\n![image](https://user-images.githubusercontent.com/89479282/233319494-b978d131-5e9d-4858-b3ae-b254692bbd12.png)\n對於GPT-2_large，第33層神經元4142與\"i\"配對在這個度量上得分最高。查看 Neuroscope [註6]確認了這個關聯：\n\n![image](https://user-images.githubusercontent.com/89479282/233319606-3e8f5dc0-43cf-47cb-aecd-33dd83d7334e.png)\n當我們繪製前5名最高得分者的最高預測比例與激發圖時：[註7]\n\n![image](https://user-images.githubusercontent.com/89479282/233319772-b1fafa3e-eb7f-4379-8abb-adcdd0065ada.png)\n我們確實看到了每對之間的強相關性！\n\n### 這一切意味著什麼？\n神經元與token的一致性是否真的衡量了神經元預測該token的程度？我們不知道。即使具有高互斥一致性的token神經元對也可能並不總是具有相關性，原因可能有幾個：\n\n1. token也可能由一組一致性較低的神經元共同預測。\n2. token可能由注意力頭（attention heads）預測。\n3. 即使神經元的激發與 token 的 logit 具有高度相關性，它也可能間接地與其他 token 的 logit 相關，以至於神經元的激發與 token 出現的機率不具有相關性。\n4. 可能有後續層往殘差流中添加相反方向，從而抵消神經元的效果。\n\n雖然是這樣說，但我們發現具有最高互斥一致性的前5名的神經元確實對上 token 預測具有很強的相關性。\n\n### TL; DR\n1. 我們在神經元級別上使用激發修補（activation patching）找到了一個對於預測特定提示中的token \" an\"非常重要的神經元。\n2. “ an\"神經元的激發與\" an\" token 的預測有一般相關性。\n3. 這可能是因為神經元的輸出權重與\" an\" token具有很高的內積（神經元與 token 有高度一致性）。此外，這個神經元與這個 token 的內積比其他任何 token 都要高。而且，這個 token 與這個神經元的內積比 token 與其他任何神經元的內積都要高（它們具有高度互斥的一致性）。\n4. 神經元與 token 之間的一致性很酷。我們找到了前5名具有互斥一致性的 神經元-token 對。這些神經元的激發與它們各自的 token 預測密切相關。\n\n重現我們結果的程式碼：https://github.com/UFO-101/an-neuron\n\n這是我們在[Apart Research](https://apartresearch.com/)的[黑客松](https://itch.io/jam/mechint)中獲勝所提交的文章做的延伸。感謝倫敦[EA Hub](https://krischari.notion.site/Our-Coworking-Space-daff577809a84832a0d8bb28940c78c0)讓我們使用他們的共享工作空間，感謝[Fazl Barez的評論](https://fbarez.github.io/)和 [Neel Nanda的建議](https://www.neelnanda.io/)，以及 [Neuroscope](https://neuroscope.io/)、[pile-10k資料集](https://huggingface.co/datasets/NeelNanda/pile-10k)和[TransformerLens](https://github.com/neelnanda-io/TransformerLens)。\n\n------\n{% notel default 註解 %}\n##### [註1]\nNeel Nanda 對 MLP 第0層的看法：\n\"通常在GPT-2 Small上觀察到，MLP 第0層非常重要，抑制它會徹底破壞LLM的性能。我目前最好的猜測是，第一個MLP層基本上是 prompt 嵌入的擴展（無論出於什麼原因），當後續層想要訪問輸入 token 時，它們主要讀取的是第一個 ML 層的輸出，而不是 prompt。在這個框架下，第一個注意力層幾乎沒有什麼作用。\n在這個框架下，MLP 第0層在第二個主題 token 上很重要，因為這是唯一一個位置有不同的輸入 token！\n我不完全確定為什麼會這樣，但我的猜測是，這是因為在GPT-2 Small中，嵌入和去嵌入矩陣是相同的。這是相當不合理的，因為嵌入和去嵌入token的任務並不是互逆的，但這是常見的做法，模型可能希望將一些參數用於克服這一點。\n我只有暗示性的證據，希望有人能夠好好研究這個問題\"\n\n##### [註2]\n神經元還有哪些可能的作用？它可能抑制了\" a\"的 logit，這對 logit 差異會產生相同的影響。或者它可能向殘差流中添加了完全不同的方向，導致後續層的神經元增加了\" an\" logit。\n\n##### [註3]\n值得注意的是，\" though\"神經元與一組語義相似的token一致，而\" an\"神經元與一組語法相似的token相關（例如，\" an\"和\" Ancients\"）。\n\n##### [註4]\n為什麼\" an\"具有更乾淨的相關性，儘管有其他一致的token(\"an\", \"An\", \" An\")？我們不能確定。一個可能的解釋是，\"An\"和\" An\"只是非常不常見的 token，所以它們對對相關性的影響很小，而\"an\"與此神經元的一致性遠低於其他三者。\n\n一般來說，我們只查看每個token的前2名神經元差異來找到的神經元，通常不會與其相對應的token具有乾淨的相關性，因為這些神經元可能同時與多個token有一致關係。\n\n##### [註5]\n當我們觀察每個token的最一致神經元時，我們可以看到了一些非常高一致性的熟悉的[麻煩製造者](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)：\n\n![image](https://user-images.githubusercontent.com/89479282/233886186-94185a3f-d7da-4c88-8a87-bdb14572dfe9.png)\n\n起初，看起來這些\"禁止的token\"都與一個\"禁止的神經元\"（第35層神經元3354）相關，它們都與該神經元非常一致。但實際上，如果我們繪製許多其他神經元的最一致token，我們也會看到一些這樣的奇怪token位於排名靠前的位置。我們的初步假設是，這可能與[hubness effect(集束效應)](https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/?commentId=M2uAwsCus2adqQsGc)有關。\n\n##### [註6]\nNeuroscope 數據對於這個神經元並不可用，所以我們從 pile-10k 資料集中選擇了最大激發數據集案例。文本1、2、3分別是提示1755、8528和6375。\n\n[註7]\n值得注意的是，前5個token之一是\"an\"，但這與我們之前提到的\" an\"不同，它很少作為一個單詞的開頭或單獨使用的單詞。同樣，與之配對的神經元，第34層神經元4549，也不是前面提到的\" an\"神經元。\n{% endnotel %}\n\n","source":"_posts/gpt2-predicted-token.md","raw":"---\ntitle: \"類神經網路中神經元對特定Token預測的影響\"\ndescription: \"深入了解GPT語言模型如何通過類神經網路中的神經元來預測Token。本文將探討神經元與Token之間的一致性及互斥一致性，揭示它們在語言理解與生成過程中的重要性\"\ndate: 2023-04-17T16:19:00+08:00\ndraft: false\ntags: [gpt, gpt-2, ChatGPT, 類神經網路, 大型語言模型]\nmathjax: true\n---\n\n> 本文翻譯自 [We Found An Neuron in GPT-2](https://clementneo.com/posts/2023/02/11/we-found-an-neuron)，作者Clement Neo 是一位倫敦的大學生。這篇文章源自他在黑客松中的題目延伸，早在2月初我就發現這篇文章相當有趣並決定翻譯它，但開學後到現在才趁空檔完成。本文像一部偵探故事，逐步揭示LLM在處理特定token時所涉及的神經元，以及如何判定這些神經元是否確實負責預測相應的token。\n\n## 我們在 GPT-2 中找到了\"an\"神經元\n`作者：Joseph Miller、Clement Neo`\n\n這個研究始於一個問題：***GPT-2 是如何知道何時該使用 \"an\" 而不是 \"a\" ？***以人類來說，這個選擇取決於後面的單詞是否以母音為開頭，但 GPT-2 一次只能輸出一個單詞（準確來說，1個 token），他是如何判斷的？\n\n雖然我們還沒有完整的答案，但我們確實在 GPT-2 Large 模型中找到了一個對於 gpt 預測 \" an\" 這個 token 至關重要的單個 MLP 神經元。同時，我們也發現這個神經元的權重與 \" an\" token 的嵌入相對應，這讓我們能夠以相同方法找到其他也能預測特定 token 的神經元。\n\n### 發現神經元\n\n##### 選擇提示詞(prompt)\n想出一個能讓 GPT-2 輸出 \" an\"（前面的空格是token的一部分）作為最佳預測的提示(prompt)是非常困難。我們實驗後最後放棄了 GPT-2_small 模型，轉向 GPT-2_large。稍後我們將會看到，即使是 GPT-2_large 也會系統性地低估 \" an\" 這個 token。這可能是因為較小的語言模型依賴於 \" a\" 的頻率更高，更有可能做出最佳猜測。我們最終找到的能讓 \" an\" 的機率達到 64% 的提示是：\n\n> “I climbed up the pear tree and picked a pear. I climbed up the `apple` tree and picked”\n\n第一句是必要的，因為它可以讓模型朝著不定冠詞的預測方向前進，如果沒有第一句，模型會做出其他預測，例如: \"[picked] up\"\n\n在繼續之前，讓我們快速回顧一下 [Transformer 架構](https://transformer-circuits.pub/2021/framework/index.html) 。每個注意力區塊和 MLP 都會將輸入加到殘差流(residual stream)中。\n   ![image](https://user-images.githubusercontent.com/89479282/232722119-09e06fca-baab-48e6-b887-4ca3c91cdd48.png)\n\n#### Logit 透鏡(Logit Lens)\n使用 [logit 透鏡](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) 這種技巧，我們在模型的每層之間從殘差流中取出 logit，並繪製 logit(\" an\") 和 logit(\" a\") 之間的差異。我們在第 31 層的 MLP 之後發現了一個大的峰值，如下圖。\n\n\n   ![image](https://user-images.githubusercontent.com/89479282/232725226-748c4121-8afe-49bf-8e03-49cd58f9d083.png)\n\n\n#### 層次激發修補(Activation Patching by the Layer)\n激發修補是一種由 [Meng 等人 (2022)](https://arxiv.org/abs/2202.05262) 提出的技術，用於分析 Transformer 中單層的重要性。在這個技術中，我們會先將一開始提到的提示運行於模型時，然後我們會將每一層的激發保存起來又稱“淨激發(clean activation)”。\n\n接著，我們將提示詞中蘋果樹(apple tree)改成檸檬樹(lemon tree)並丟給模型：\n\n> “I climbed up the pear tree and picked a pear. I climbed up the `apple` tree and picked”\n\n> “I climbed up the pear tree and picked a pear. I climbed up the `lemon` tree and picked”\n\n通過將 \"apple\" 替換為 \"lemon\"，我們誘使模型預測 \" a\" 而非 \" an\"。當模型預測 \" a\" 而非 \" an\" 時，我們可以將當前這層中的激活替換為其淨激活，以查看模型朝向 \" an\" token移動的程度，這可以代表該前層對於預測 \" an\" 的重要性。我們對模型中的所有層重複這個過程。\n\n![image](https://user-images.githubusercontent.com/89479282/232732236-84a664ab-c26f-430b-80b7-882819a83ba2.png)\n\n![image](https://user-images.githubusercontent.com/89479282/232732433-dd91af7e-3450-4b34-a1a9-76b3fa84d728.png)\n\n這篇文章接下來我們不會談太多關於 Attention layer 的部分，但這些結果表明，第 26 層是 \" picked\" 開始大量考慮 \" apple\" 的地方，這顯然是預測 \" an\" 所必需要具備的。\n\n注意：圖表上的刻度是相對的邏輯差異恢復程度(relative logit difference recovery)\n\n$$\n\\frac{PatchedLogitDiff − CorruptedLogitDiff}{CleanLogitDiff − CorruptedLogitDiff}\n$$\n\n(也就是說，圖代表有多少修補恢復原先提示中\" an\"和\" a\"之間的 logit 比例）\n\n![image](https://user-images.githubusercontent.com/89479282/232733478-84b5de26-3ddc-42ca-ac8f-ea82d8bd2794.png)\n\n兩個最突出的 MLP 層是第 0 層和第 31 層。我們已經知道第 0 層的 MLP 對於 GPT-2 的運作非常重要（儘管我們不確定為什麼第 0 層的注意力很重要）[註1]。第 31 層的效果更有趣。我們的結果表明，第 31 層的 MLP 在預測 \" an\" token 方面有著顯著的作用。（如果你對此結果如何與上面的 logit 透鏡協作感到困惑，你可以參考此處[評論](https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2?commentId=FLpxtfnwnMjZwXv3B#comments))\n\n### 發現一： <br />我們可以通過對單個神經元進行激發修補(activation patching)來發現具有預測性的神經元\n\n激發修補(activation patching)已被用來按層研究 Transformer ，但我們能否將這種技術推向更遠，將其應用到單個神經元？由於 Transformer 中的每個 MLP 只有一個隱藏層，因此每個神經元的激活都不會影響 MLP 中的其他神經元。所以，我們應該能夠對單個神經元進行修補，因為它們在同一層的 Attention heads 之間是相互獨立的。\n\n我們以與上述提到的分層修補中類似的方法對第 31 層 MLP 的神經元進行神經元級別的激發修補(activation patching)。當我們將被修改過的提示運行於模型時，在 MLP 中重新引入每個神經元的淨激發(clean activation)，並觀察恢復每個神經元對 \" a\" 和 \" an\" 之間 logit 差異的貢獻。\n\n![image](https://user-images.githubusercontent.com/89479282/233306391-41c0f3f6-7dec-4b63-b9f7-48fb11d7c492.png)\n\n我們可以發現修補神經元 892 可以恢復最初提示中 50% 的 logit 差異，而修補整個層實際上只恢復了 49% 的表現上更差。\n\n\n總之，我們在 GPT-2 Large 中找到了一個對預測 \" an\" token 至關重要的單個 MLP 神經元。通過啟動修補技術，我們可以繼續研究變換器中單個神經元的重要性，進一步了解它們在預測特定 token 方面的作用。雖然我們尚未完全解答開篇提出的問題，但這些發現將有助於我們更深入地理解語言模型的運作機制。\n\n### 發現二：「an-神經元」的激發與「an」token 預測相關性\n\n#### Neuroscope [第31層的892號神經元](https://neuroscope.io/gpt2-large/31/892.html)最大激發案例\n\n![image](https://user-images.githubusercontent.com/89479282/233309645-a7705dd3-c1c6-449d-a39c-0690357067b6.png)\nNeuroscope 是用來查看 GPT-2 中每個神經元在大型資料集中最大激發案例的線上工具。當我們觀察第31層，編號892神經元時，我們發現這個神經元在後續 token 是「an」的情況下達到最大的激發(maximally activates)。\n\n但 Neuroscope 只顯示出最高激發的前 20 個範例。我們想知道的是在更廣泛的激發範圍內，神經元與 token 是否存在相關性？\n\n#### 在更大的數據集上測試神經元\n為了確認這一點，我們將 pile-10k 資料集運行在模型中。這是一個多樣化的資料集，包含了大約 1000 萬個來自 The Pile 的標記，分成 1024 個標記的提示。我們繪製了神經元激活範圍內「an」預測的比例：\n\n![image](https://user-images.githubusercontent.com/89479282/233310982-62139036-2b3f-4a48-9d1a-4b46b65b79ed.png)\n我們看到「an」 token 的預測率隨著神經元激發數增加而增加，甚至到達總是最高預測的程度。趨勢中可以看出有些噪聲，這暗示模型中可能還有其他機制也對「an」的預測有所影響。或者當「an」的 logit 增加時，其他 logit 也會同時增加。\n\n值得注意的是，儘管資料集中實際出現了 12,000 次「an」，但模型只預測了 1,500 次「an」。難怪找到一個好的提示這麼困難！\n\n#### 神經元的輸出權重(output weights)與「an」token 的內積很高\n神經元如何影響模型的輸出？有趣的是，神經元的輸出權重與「an」token 的嵌入有很高的內積。我們將這個內積稱為這個神經元與 token 的一致性(congruence)。與其他token相比（如「any」和「had」），神經元的一致性在「an」這個token中表現的非常高：\n\n![image](https://user-images.githubusercontent.com/89479282/233312469-290d6f35-d93f-478d-9597-1ad12848c5ef.png)\n事實上，當我們計算神經元與所有 token 的一致性(congruence)時，有一些明顯的異常值：\n\n![image](https://user-images.githubusercontent.com/89479282/233313512-0a4b2991-f948-48c4-924e-0d0458fdd686.png)\n看起來這個神經元基本上是將「an」的嵌入添加到殘差流(residual stream)中，這增加了「an」的輸出機率，因為移除嵌入的步驟包括將最終殘差與每個token的內積相乘。[註2]\n\n還有其他神經元也與「an」一致嗎？為了找出答案，我們繪製了所有神經元與「an」標記的一致性圖：\n![image](https://user-images.githubusercontent.com/89479282/233314247-d93a0938-8b36-4992-be48-6cd93076e922.png)\n\n我們的神經元(31-892)的一致性遠高於其他神經元，但還有一些其他神經元具有相當高的一致性(congruence)。這些其他神經元可能是為什麼「an」神經元激發時與「an」token 預測之間相關性不完美的原因之一：有可能存在其他提示(prompt)有著「an」，而模型是使用其他神經元來完成該提示的「an」token。\n\n如果是這樣的話，我們能否使用一致性來找到與單個 token 預測有完美相關性的神經元？\n\n### 發現三：我們可以使用神經元的輸出一致性來找到預測 token 的特定神經元\n#### 找到與特定 token 相關的神經元\n\n我們可以透過以下步驟搜索來找到與特定 token 相關的神經元：\n\n1. 對於每個 token，找到具有最高輸出一致性(congruence)的神經元\n2. 對於這其中的每一個神經元，比較預測同一個 token 中擁有最高一致性和第二高的神經元\n3. 找到擁有 最獨特一致性(the most exclusively congruent)的神經元。\n通過這種搜索，我們希望找到對特殊 token 具有影響力的神經元。我們的猜想是，這些神經元的激發與其 token 的預測更相關，因為該 token 的任何預測都會「依賴」於該神經元。\n\n![image](https://user-images.githubusercontent.com/89479282/233317704-3d37d708-ef82-4249-a080-0b7257ff539e.png)\n\n讓我們試著跑一次「though」神經元 - 第 28 層神經元 1921 - 看我們是否能得到更乾淨的統計圖\n![image](https://user-images.githubusercontent.com/89479282/233318279-2d83bf69-9dcd-4831-86d3-18410b827a08.png)\n\n哇，這比\" an\"神經元的圖要亂多了。發生了什麼事？\n\n我們來看看 Neuroscope 提供的神經元數據，發現這個神經元負責預測了\" though\"和\" however\"這兩個token。這讓事情變得更複雜，因為看來這個神經元是與一組語義相似的 token（連接副詞）相關。\n[註3]\n![image](https://user-images.githubusercontent.com/89479282/233319211-2922b2fc-230d-4c48-b0eb-142eb88e5b53.png)\n\n當我們計算神經元與所有 token 的一致性時，我們發現類似的 token 經常作為異常值出現：\n![image](https://user-images.githubusercontent.com/89479282/233319375-96407240-e7b3-4544-a0e8-645b4f8dc136.png)\n\n在上面的大型數據集相關圖中，神經元激發並且預測\" however\"這個 token 的一致性被預測\" though\" token 超過的狀況是一個負面的案例，因為\" though\" 不是這個神經元最高預測 token 。這也可以解釋\" an\"相關中的一些噪聲，因為與\"An\"、\" An\"和\"an\"具有相同的意思，這有可能是噪聲的來源。[註4]\n\n我們能找到一個更簡單的神經元來研究嗎？最好是一個只預測單個 token 的神經元。\n\n#### 找到一個乾淨的關聯神經元\n要使神經元與 token \"乾淨地關聯\"，它們之間的一致性應該是互斥的，這意味著：\n\n1. 神經元與token的一致性遠大於與其他任何神經元的一致性。\n2. 神經元與token的一致性遠大於與其他任何token的一致性。\n（請記住，\"一致性(congruence)\"僅是我們表達內積的術語。）\n\n這兩個標準都有助於簡化神經元與其 token 之間的關係。如果神經元與 token 的一致性是表示神經元對該 token 預測的貢獻程度，第一條規則是確保「只有這個神經元」負責預測該token，而第二個規則則可以確保這個神經元「只會負責預測該token」。\n\n我們的搜索如下：\n\n1. **對於每個 token，找到最一致的神經元。**\n2. **對於每個神經元，找到最一致的 token。**[註5]\n3. **找到兩個列表上的token神經元對，即神經元最一致的token是與該神經元最一致的token！**\n4. **通過將它們的前兩個token一致性差與前兩個神經元一致性差相乘，計算它們之間的差異程度。**\n5. **找到具有最高互斥一致性的對。**\n\n![image](https://user-images.githubusercontent.com/89479282/233319494-b978d131-5e9d-4858-b3ae-b254692bbd12.png)\n對於GPT-2_large，第33層神經元4142與\"i\"配對在這個度量上得分最高。查看 Neuroscope [註6]確認了這個關聯：\n\n![image](https://user-images.githubusercontent.com/89479282/233319606-3e8f5dc0-43cf-47cb-aecd-33dd83d7334e.png)\n當我們繪製前5名最高得分者的最高預測比例與激發圖時：[註7]\n\n![image](https://user-images.githubusercontent.com/89479282/233319772-b1fafa3e-eb7f-4379-8abb-adcdd0065ada.png)\n我們確實看到了每對之間的強相關性！\n\n### 這一切意味著什麼？\n神經元與token的一致性是否真的衡量了神經元預測該token的程度？我們不知道。即使具有高互斥一致性的token神經元對也可能並不總是具有相關性，原因可能有幾個：\n\n1. token也可能由一組一致性較低的神經元共同預測。\n2. token可能由注意力頭（attention heads）預測。\n3. 即使神經元的激發與 token 的 logit 具有高度相關性，它也可能間接地與其他 token 的 logit 相關，以至於神經元的激發與 token 出現的機率不具有相關性。\n4. 可能有後續層往殘差流中添加相反方向，從而抵消神經元的效果。\n\n雖然是這樣說，但我們發現具有最高互斥一致性的前5名的神經元確實對上 token 預測具有很強的相關性。\n\n### TL; DR\n1. 我們在神經元級別上使用激發修補（activation patching）找到了一個對於預測特定提示中的token \" an\"非常重要的神經元。\n2. “ an\"神經元的激發與\" an\" token 的預測有一般相關性。\n3. 這可能是因為神經元的輸出權重與\" an\" token具有很高的內積（神經元與 token 有高度一致性）。此外，這個神經元與這個 token 的內積比其他任何 token 都要高。而且，這個 token 與這個神經元的內積比 token 與其他任何神經元的內積都要高（它們具有高度互斥的一致性）。\n4. 神經元與 token 之間的一致性很酷。我們找到了前5名具有互斥一致性的 神經元-token 對。這些神經元的激發與它們各自的 token 預測密切相關。\n\n重現我們結果的程式碼：https://github.com/UFO-101/an-neuron\n\n這是我們在[Apart Research](https://apartresearch.com/)的[黑客松](https://itch.io/jam/mechint)中獲勝所提交的文章做的延伸。感謝倫敦[EA Hub](https://krischari.notion.site/Our-Coworking-Space-daff577809a84832a0d8bb28940c78c0)讓我們使用他們的共享工作空間，感謝[Fazl Barez的評論](https://fbarez.github.io/)和 [Neel Nanda的建議](https://www.neelnanda.io/)，以及 [Neuroscope](https://neuroscope.io/)、[pile-10k資料集](https://huggingface.co/datasets/NeelNanda/pile-10k)和[TransformerLens](https://github.com/neelnanda-io/TransformerLens)。\n\n------\n{% notel default 註解 %}\n##### [註1]\nNeel Nanda 對 MLP 第0層的看法：\n\"通常在GPT-2 Small上觀察到，MLP 第0層非常重要，抑制它會徹底破壞LLM的性能。我目前最好的猜測是，第一個MLP層基本上是 prompt 嵌入的擴展（無論出於什麼原因），當後續層想要訪問輸入 token 時，它們主要讀取的是第一個 ML 層的輸出，而不是 prompt。在這個框架下，第一個注意力層幾乎沒有什麼作用。\n在這個框架下，MLP 第0層在第二個主題 token 上很重要，因為這是唯一一個位置有不同的輸入 token！\n我不完全確定為什麼會這樣，但我的猜測是，這是因為在GPT-2 Small中，嵌入和去嵌入矩陣是相同的。這是相當不合理的，因為嵌入和去嵌入token的任務並不是互逆的，但這是常見的做法，模型可能希望將一些參數用於克服這一點。\n我只有暗示性的證據，希望有人能夠好好研究這個問題\"\n\n##### [註2]\n神經元還有哪些可能的作用？它可能抑制了\" a\"的 logit，這對 logit 差異會產生相同的影響。或者它可能向殘差流中添加了完全不同的方向，導致後續層的神經元增加了\" an\" logit。\n\n##### [註3]\n值得注意的是，\" though\"神經元與一組語義相似的token一致，而\" an\"神經元與一組語法相似的token相關（例如，\" an\"和\" Ancients\"）。\n\n##### [註4]\n為什麼\" an\"具有更乾淨的相關性，儘管有其他一致的token(\"an\", \"An\", \" An\")？我們不能確定。一個可能的解釋是，\"An\"和\" An\"只是非常不常見的 token，所以它們對對相關性的影響很小，而\"an\"與此神經元的一致性遠低於其他三者。\n\n一般來說，我們只查看每個token的前2名神經元差異來找到的神經元，通常不會與其相對應的token具有乾淨的相關性，因為這些神經元可能同時與多個token有一致關係。\n\n##### [註5]\n當我們觀察每個token的最一致神經元時，我們可以看到了一些非常高一致性的熟悉的[麻煩製造者](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)：\n\n![image](https://user-images.githubusercontent.com/89479282/233886186-94185a3f-d7da-4c88-8a87-bdb14572dfe9.png)\n\n起初，看起來這些\"禁止的token\"都與一個\"禁止的神經元\"（第35層神經元3354）相關，它們都與該神經元非常一致。但實際上，如果我們繪製許多其他神經元的最一致token，我們也會看到一些這樣的奇怪token位於排名靠前的位置。我們的初步假設是，這可能與[hubness effect(集束效應)](https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/?commentId=M2uAwsCus2adqQsGc)有關。\n\n##### [註6]\nNeuroscope 數據對於這個神經元並不可用，所以我們從 pile-10k 資料集中選擇了最大激發數據集案例。文本1、2、3分別是提示1755、8528和6375。\n\n[註7]\n值得注意的是，前5個token之一是\"an\"，但這與我們之前提到的\" an\"不同，它很少作為一個單詞的開頭或單獨使用的單詞。同樣，與之配對的神經元，第34層神經元4549，也不是前面提到的\" an\"神經元。\n{% endnotel %}\n\n","slug":"gpt2-predicted-token","published":1,"updated":"2023-05-10T05:56:25.996Z","_id":"clgugpjbs0000c1x196n3cgv3","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>本文翻譯自 <a class=\"link\" href=\"https://clementneo.com/posts/2023/02/11/we-found-an-neuron\">We Found An Neuron in GPT-2 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>，作者Clement Neo 是一位倫敦的大學生。這篇文章源自他在黑客松中的題目延伸，早在2月初我就發現這篇文章相當有趣並決定翻譯它，但開學後到現在才趁空檔完成。本文像一部偵探故事，逐步揭示LLM在處理特定token時所涉及的神經元，以及如何判定這些神經元是否確實負責預測相應的token。</p>\n</blockquote>\n<h2 id=\"我們在-GPT-2-中找到了”an”神經元\"><a href=\"#我們在-GPT-2-中找到了”an”神經元\" class=\"headerlink\" title=\"我們在 GPT-2 中找到了”an”神經元\"></a>我們在 GPT-2 中找到了”an”神經元</h2><p><code>作者：Joseph Miller、Clement Neo</code></p>\n<p>這個研究始於一個問題：<em><strong>GPT-2 是如何知道何時該使用 “an” 而不是 “a” ？</strong></em>以人類來說，這個選擇取決於後面的單詞是否以母音為開頭，但 GPT-2 一次只能輸出一個單詞（準確來說，1個 token），他是如何判斷的？</p>\n<p>雖然我們還沒有完整的答案，但我們確實在 GPT-2 Large 模型中找到了一個對於 gpt 預測 “ an” 這個 token 至關重要的單個 MLP 神經元。同時，我們也發現這個神經元的權重與 “ an” token 的嵌入相對應，這讓我們能夠以相同方法找到其他也能預測特定 token 的神經元。</p>\n<h3 id=\"發現神經元\"><a href=\"#發現神經元\" class=\"headerlink\" title=\"發現神經元\"></a>發現神經元</h3><h5 id=\"選擇提示詞-prompt\"><a href=\"#選擇提示詞-prompt\" class=\"headerlink\" title=\"選擇提示詞(prompt)\"></a>選擇提示詞(prompt)</h5><p>想出一個能讓 GPT-2 輸出 “ an”（前面的空格是token的一部分）作為最佳預測的提示(prompt)是非常困難。我們實驗後最後放棄了 GPT-2_small 模型，轉向 GPT-2_large。稍後我們將會看到，即使是 GPT-2_large 也會系統性地低估 “ an” 這個 token。這可能是因為較小的語言模型依賴於 “ a” 的頻率更高，更有可能做出最佳猜測。我們最終找到的能讓 “ an” 的機率達到 64% 的提示是：</p>\n<blockquote>\n<p>“I climbed up the pear tree and picked a pear. I climbed up the <code>apple</code> tree and picked”</p>\n</blockquote>\n<p>第一句是必要的，因為它可以讓模型朝著不定冠詞的預測方向前進，如果沒有第一句，模型會做出其他預測，例如: “[picked] up”</p>\n<p>在繼續之前，讓我們快速回顧一下 <a class=\"link\" href=\"https://transformer-circuits.pub/2021/framework/index.html\">Transformer 架構 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 。每個注意力區塊和 MLP 都會將輸入加到殘差流(residual stream)中。<br>   <img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232722119-09e06fca-baab-48e6-b887-4ca3c91cdd48.png\" alt=\"image\"></p>\n<h4 id=\"Logit-透鏡-Logit-Lens\"><a href=\"#Logit-透鏡-Logit-Lens\" class=\"headerlink\" title=\"Logit 透鏡(Logit Lens)\"></a>Logit 透鏡(Logit Lens)</h4><p>使用 <a class=\"link\" href=\"https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens\">logit 透鏡 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 這種技巧，我們在模型的每層之間從殘差流中取出 logit，並繪製 logit(“ an”) 和 logit(“ a”) 之間的差異。我們在第 31 層的 MLP 之後發現了一個大的峰值，如下圖。</p>\n<p>   <img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232725226-748c4121-8afe-49bf-8e03-49cd58f9d083.png\" alt=\"image\"></p>\n<h4 id=\"層次激發修補-Activation-Patching-by-the-Layer\"><a href=\"#層次激發修補-Activation-Patching-by-the-Layer\" class=\"headerlink\" title=\"層次激發修補(Activation Patching by the Layer)\"></a>層次激發修補(Activation Patching by the Layer)</h4><p>激發修補是一種由 <a class=\"link\" href=\"https://arxiv.org/abs/2202.05262\">Meng 等人 (2022) <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 提出的技術，用於分析 Transformer 中單層的重要性。在這個技術中，我們會先將一開始提到的提示運行於模型時，然後我們會將每一層的激發保存起來又稱“淨激發(clean activation)”。</p>\n<p>接著，我們將提示詞中蘋果樹(apple tree)改成檸檬樹(lemon tree)並丟給模型：</p>\n<blockquote>\n<p>“I climbed up the pear tree and picked a pear. I climbed up the <code>apple</code> tree and picked”</p>\n</blockquote>\n<blockquote>\n<p>“I climbed up the pear tree and picked a pear. I climbed up the <code>lemon</code> tree and picked”</p>\n</blockquote>\n<p>通過將 “apple” 替換為 “lemon”，我們誘使模型預測 “ a” 而非 “ an”。當模型預測 “ a” 而非 “ an” 時，我們可以將當前這層中的激活替換為其淨激活，以查看模型朝向 “ an” token移動的程度，這可以代表該前層對於預測 “ an” 的重要性。我們對模型中的所有層重複這個過程。</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232732236-84a664ab-c26f-430b-80b7-882819a83ba2.png\" alt=\"image\"></p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232732433-dd91af7e-3450-4b34-a1a9-76b3fa84d728.png\" alt=\"image\"></p>\n<p>這篇文章接下來我們不會談太多關於 Attention layer 的部分，但這些結果表明，第 26 層是 “ picked” 開始大量考慮 “ apple” 的地方，這顯然是預測 “ an” 所必需要具備的。</p>\n<p>注意：圖表上的刻度是相對的邏輯差異恢復程度(relative logit difference recovery)</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\" display=\"true\"><svg style=\"vertical-align: -2.016ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"43.239ex\" height=\"5.14ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1381 19111.4 2272\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mfrac\"><g data-mml-node=\"mrow\" transform=\"translate(220,676)\"><g data-mml-node=\"mi\"><path data-c=\"1D443\" d=\"M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(751,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1280,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1641,0)\"><path data-c=\"1D450\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2074,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2650,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3116,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3636,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4317,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4802,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5279,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5624,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5985,0)\"><path data-c=\"1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6813,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(7158,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(7708,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(8480.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9480.4,0)\"><path data-c=\"1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10240.4,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10725.4,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11176.4,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11627.4,0)\"><path data-c=\"1D462\" d=\"M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12199.4,0)\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12702.4,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13063.4,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13529.4,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(14049.4,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(14730.4,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(15215.4,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(15692.4,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(16037.4,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(16398.4,0)\"><path data-c=\"1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(17226.4,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(17571.4,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(18121.4,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g></g><g data-mml-node=\"mrow\" transform=\"translate(711.5,-686)\"><g data-mml-node=\"mi\"><path data-c=\"1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(760,0)\"><path data-c=\"1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1058,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1524,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2053,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2653,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3334,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3819,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4296,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4641,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5002,0)\"><path data-c=\"1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5830,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6175,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6725,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(7497.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(8497.4,0)\"><path data-c=\"1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9257.4,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9742.4,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10193.4,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10644.4,0)\"><path data-c=\"1D462\" d=\"M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11216.4,0)\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11719.4,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12080.4,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12546.4,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13066.4,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13747.4,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(14232.4,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(14709.4,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(15054.4,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(15415.4,0)\"><path data-c=\"1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(16243.4,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(16588.4,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(17138.4,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g></g><rect width=\"18871.4\" height=\"60\" x=\"120\" y=\"220\"></rect></g></g></g></svg></mjx-container></p>\n<p>(也就是說，圖代表有多少修補恢復原先提示中” an”和” a”之間的 logit 比例）</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232733478-84b5de26-3ddc-42ca-ac8f-ea82d8bd2794.png\" alt=\"image\"></p>\n<p>兩個最突出的 MLP 層是第 0 層和第 31 層。我們已經知道第 0 層的 MLP 對於 GPT-2 的運作非常重要（儘管我們不確定為什麼第 0 層的注意力很重要）[註1]。第 31 層的效果更有趣。我們的結果表明，第 31 層的 MLP 在預測 “ an” token 方面有著顯著的作用。（如果你對此結果如何與上面的 logit 透鏡協作感到困惑，你可以參考此處<a class=\"link\" href=\"https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2?commentId=FLpxtfnwnMjZwXv3B#comments\">評論 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>)</p>\n<h3 id=\"發現一：-我們可以通過對單個神經元進行激發修補-activation-patching-來發現具有預測性的神經元\"><a href=\"#發現一：-我們可以通過對單個神經元進行激發修補-activation-patching-來發現具有預測性的神經元\" class=\"headerlink\" title=\"發現一： 我們可以通過對單個神經元進行激發修補(activation patching)來發現具有預測性的神經元\"></a>發現一： <br>我們可以通過對單個神經元進行激發修補(activation patching)來發現具有預測性的神經元</h3><p>激發修補(activation patching)已被用來按層研究 Transformer ，但我們能否將這種技術推向更遠，將其應用到單個神經元？由於 Transformer 中的每個 MLP 只有一個隱藏層，因此每個神經元的激活都不會影響 MLP 中的其他神經元。所以，我們應該能夠對單個神經元進行修補，因為它們在同一層的 Attention heads 之間是相互獨立的。</p>\n<p>我們以與上述提到的分層修補中類似的方法對第 31 層 MLP 的神經元進行神經元級別的激發修補(activation patching)。當我們將被修改過的提示運行於模型時，在 MLP 中重新引入每個神經元的淨激發(clean activation)，並觀察恢復每個神經元對 “ a” 和 “ an” 之間 logit 差異的貢獻。</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233306391-41c0f3f6-7dec-4b63-b9f7-48fb11d7c492.png\" alt=\"image\"></p>\n<p>我們可以發現修補神經元 892 可以恢復最初提示中 50% 的 logit 差異，而修補整個層實際上只恢復了 49% 的表現上更差。</p>\n<p>總之，我們在 GPT-2 Large 中找到了一個對預測 “ an” token 至關重要的單個 MLP 神經元。通過啟動修補技術，我們可以繼續研究變換器中單個神經元的重要性，進一步了解它們在預測特定 token 方面的作用。雖然我們尚未完全解答開篇提出的問題，但這些發現將有助於我們更深入地理解語言模型的運作機制。</p>\n<h3 id=\"發現二：「an-神經元」的激發與「an」token-預測相關性\"><a href=\"#發現二：「an-神經元」的激發與「an」token-預測相關性\" class=\"headerlink\" title=\"發現二：「an-神經元」的激發與「an」token 預測相關性\"></a>發現二：「an-神經元」的激發與「an」token 預測相關性</h3><h4 id=\"Neuroscope-第31層的892號神經元最大激發案例\"><a href=\"#Neuroscope-第31層的892號神經元最大激發案例\" class=\"headerlink\" title=\"Neuroscope 第31層的892號神經元最大激發案例\"></a>Neuroscope <a class=\"link\" href=\"https://neuroscope.io/gpt2-large/31/892.html\">第31層的892號神經元 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>最大激發案例</h4><p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233309645-a7705dd3-c1c6-449d-a39c-0690357067b6.png\" alt=\"image\"><br>Neuroscope 是用來查看 GPT-2 中每個神經元在大型資料集中最大激發案例的線上工具。當我們觀察第31層，編號892神經元時，我們發現這個神經元在後續 token 是「an」的情況下達到最大的激發(maximally activates)。</p>\n<p>但 Neuroscope 只顯示出最高激發的前 20 個範例。我們想知道的是在更廣泛的激發範圍內，神經元與 token 是否存在相關性？</p>\n<h4 id=\"在更大的數據集上測試神經元\"><a href=\"#在更大的數據集上測試神經元\" class=\"headerlink\" title=\"在更大的數據集上測試神經元\"></a>在更大的數據集上測試神經元</h4><p>為了確認這一點，我們將 pile-10k 資料集運行在模型中。這是一個多樣化的資料集，包含了大約 1000 萬個來自 The Pile 的標記，分成 1024 個標記的提示。我們繪製了神經元激活範圍內「an」預測的比例：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233310982-62139036-2b3f-4a48-9d1a-4b46b65b79ed.png\" alt=\"image\"><br>我們看到「an」 token 的預測率隨著神經元激發數增加而增加，甚至到達總是最高預測的程度。趨勢中可以看出有些噪聲，這暗示模型中可能還有其他機制也對「an」的預測有所影響。或者當「an」的 logit 增加時，其他 logit 也會同時增加。</p>\n<p>值得注意的是，儘管資料集中實際出現了 12,000 次「an」，但模型只預測了 1,500 次「an」。難怪找到一個好的提示這麼困難！</p>\n<h4 id=\"神經元的輸出權重-output-weights-與「an」token-的內積很高\"><a href=\"#神經元的輸出權重-output-weights-與「an」token-的內積很高\" class=\"headerlink\" title=\"神經元的輸出權重(output weights)與「an」token 的內積很高\"></a>神經元的輸出權重(output weights)與「an」token 的內積很高</h4><p>神經元如何影響模型的輸出？有趣的是，神經元的輸出權重與「an」token 的嵌入有很高的內積。我們將這個內積稱為這個神經元與 token 的一致性(congruence)。與其他token相比（如「any」和「had」），神經元的一致性在「an」這個token中表現的非常高：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233312469-290d6f35-d93f-478d-9597-1ad12848c5ef.png\" alt=\"image\"><br>事實上，當我們計算神經元與所有 token 的一致性(congruence)時，有一些明顯的異常值：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233313512-0a4b2991-f948-48c4-924e-0d0458fdd686.png\" alt=\"image\"><br>看起來這個神經元基本上是將「an」的嵌入添加到殘差流(residual stream)中，這增加了「an」的輸出機率，因為移除嵌入的步驟包括將最終殘差與每個token的內積相乘。[註2]</p>\n<p>還有其他神經元也與「an」一致嗎？為了找出答案，我們繪製了所有神經元與「an」標記的一致性圖：<br><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233314247-d93a0938-8b36-4992-be48-6cd93076e922.png\" alt=\"image\"></p>\n<p>我們的神經元(31-892)的一致性遠高於其他神經元，但還有一些其他神經元具有相當高的一致性(congruence)。這些其他神經元可能是為什麼「an」神經元激發時與「an」token 預測之間相關性不完美的原因之一：有可能存在其他提示(prompt)有著「an」，而模型是使用其他神經元來完成該提示的「an」token。</p>\n<p>如果是這樣的話，我們能否使用一致性來找到與單個 token 預測有完美相關性的神經元？</p>\n<h3 id=\"發現三：我們可以使用神經元的輸出一致性來找到預測-token-的特定神經元\"><a href=\"#發現三：我們可以使用神經元的輸出一致性來找到預測-token-的特定神經元\" class=\"headerlink\" title=\"發現三：我們可以使用神經元的輸出一致性來找到預測 token 的特定神經元\"></a>發現三：我們可以使用神經元的輸出一致性來找到預測 token 的特定神經元</h3><h4 id=\"找到與特定-token-相關的神經元\"><a href=\"#找到與特定-token-相關的神經元\" class=\"headerlink\" title=\"找到與特定 token 相關的神經元\"></a>找到與特定 token 相關的神經元</h4><p>我們可以透過以下步驟搜索來找到與特定 token 相關的神經元：</p>\n<ol>\n<li>對於每個 token，找到具有最高輸出一致性(congruence)的神經元</li>\n<li>對於這其中的每一個神經元，比較預測同一個 token 中擁有最高一致性和第二高的神經元</li>\n<li>找到擁有 最獨特一致性(the most exclusively congruent)的神經元。<br>通過這種搜索，我們希望找到對特殊 token 具有影響力的神經元。我們的猜想是，這些神經元的激發與其 token 的預測更相關，因為該 token 的任何預測都會「依賴」於該神經元。</li>\n</ol>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233317704-3d37d708-ef82-4249-a080-0b7257ff539e.png\" alt=\"image\"></p>\n<p>讓我們試著跑一次「though」神經元 - 第 28 層神經元 1921 - 看我們是否能得到更乾淨的統計圖<br><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233318279-2d83bf69-9dcd-4831-86d3-18410b827a08.png\" alt=\"image\"></p>\n<p>哇，這比” an”神經元的圖要亂多了。發生了什麼事？</p>\n<p>我們來看看 Neuroscope 提供的神經元數據，發現這個神經元負責預測了” though”和” however”這兩個token。這讓事情變得更複雜，因為看來這個神經元是與一組語義相似的 token（連接副詞）相關。<br>[註3]<br><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319211-2922b2fc-230d-4c48-b0eb-142eb88e5b53.png\" alt=\"image\"></p>\n<p>當我們計算神經元與所有 token 的一致性時，我們發現類似的 token 經常作為異常值出現：<br><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319375-96407240-e7b3-4544-a0e8-645b4f8dc136.png\" alt=\"image\"></p>\n<p>在上面的大型數據集相關圖中，神經元激發並且預測” however”這個 token 的一致性被預測” though” token 超過的狀況是一個負面的案例，因為” though” 不是這個神經元最高預測 token 。這也可以解釋” an”相關中的一些噪聲，因為與”An”、” An”和”an”具有相同的意思，這有可能是噪聲的來源。[註4]</p>\n<p>我們能找到一個更簡單的神經元來研究嗎？最好是一個只預測單個 token 的神經元。</p>\n<h4 id=\"找到一個乾淨的關聯神經元\"><a href=\"#找到一個乾淨的關聯神經元\" class=\"headerlink\" title=\"找到一個乾淨的關聯神經元\"></a>找到一個乾淨的關聯神經元</h4><p>要使神經元與 token “乾淨地關聯”，它們之間的一致性應該是互斥的，這意味著：</p>\n<ol>\n<li>神經元與token的一致性遠大於與其他任何神經元的一致性。</li>\n<li>神經元與token的一致性遠大於與其他任何token的一致性。<br>（請記住，”一致性(congruence)”僅是我們表達內積的術語。）</li>\n</ol>\n<p>這兩個標準都有助於簡化神經元與其 token 之間的關係。如果神經元與 token 的一致性是表示神經元對該 token 預測的貢獻程度，第一條規則是確保「只有這個神經元」負責預測該token，而第二個規則則可以確保這個神經元「只會負責預測該token」。</p>\n<p>我們的搜索如下：</p>\n<ol>\n<li><strong>對於每個 token，找到最一致的神經元。</strong></li>\n<li><strong>對於每個神經元，找到最一致的 token。</strong>[註5]</li>\n<li><strong>找到兩個列表上的token神經元對，即神經元最一致的token是與該神經元最一致的token！</strong></li>\n<li><strong>通過將它們的前兩個token一致性差與前兩個神經元一致性差相乘，計算它們之間的差異程度。</strong></li>\n<li><strong>找到具有最高互斥一致性的對。</strong></li>\n</ol>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319494-b978d131-5e9d-4858-b3ae-b254692bbd12.png\" alt=\"image\"><br>對於GPT-2_large，第33層神經元4142與”i”配對在這個度量上得分最高。查看 Neuroscope [註6]確認了這個關聯：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319606-3e8f5dc0-43cf-47cb-aecd-33dd83d7334e.png\" alt=\"image\"><br>當我們繪製前5名最高得分者的最高預測比例與激發圖時：[註7]</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319772-b1fafa3e-eb7f-4379-8abb-adcdd0065ada.png\" alt=\"image\"><br>我們確實看到了每對之間的強相關性！</p>\n<h3 id=\"這一切意味著什麼？\"><a href=\"#這一切意味著什麼？\" class=\"headerlink\" title=\"這一切意味著什麼？\"></a>這一切意味著什麼？</h3><p>神經元與token的一致性是否真的衡量了神經元預測該token的程度？我們不知道。即使具有高互斥一致性的token神經元對也可能並不總是具有相關性，原因可能有幾個：</p>\n<ol>\n<li>token也可能由一組一致性較低的神經元共同預測。</li>\n<li>token可能由注意力頭（attention heads）預測。</li>\n<li>即使神經元的激發與 token 的 logit 具有高度相關性，它也可能間接地與其他 token 的 logit 相關，以至於神經元的激發與 token 出現的機率不具有相關性。</li>\n<li>可能有後續層往殘差流中添加相反方向，從而抵消神經元的效果。</li>\n</ol>\n<p>雖然是這樣說，但我們發現具有最高互斥一致性的前5名的神經元確實對上 token 預測具有很強的相關性。</p>\n<h3 id=\"TL-DR\"><a href=\"#TL-DR\" class=\"headerlink\" title=\"TL; DR\"></a>TL; DR</h3><ol>\n<li>我們在神經元級別上使用激發修補（activation patching）找到了一個對於預測特定提示中的token “ an”非常重要的神經元。</li>\n<li>“ an”神經元的激發與” an” token 的預測有一般相關性。</li>\n<li>這可能是因為神經元的輸出權重與” an” token具有很高的內積（神經元與 token 有高度一致性）。此外，這個神經元與這個 token 的內積比其他任何 token 都要高。而且，這個 token 與這個神經元的內積比 token 與其他任何神經元的內積都要高（它們具有高度互斥的一致性）。</li>\n<li>神經元與 token 之間的一致性很酷。我們找到了前5名具有互斥一致性的 神經元-token 對。這些神經元的激發與它們各自的 token 預測密切相關。</li>\n</ol>\n<p>重現我們結果的程式碼：<a class=\"link\" href=\"https://github.com/UFO-101/an-neuron\">https://github.com/UFO-101/an-neuron <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n<p>這是我們在<a class=\"link\" href=\"https://apartresearch.com/\">Apart Research <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>的<a class=\"link\" href=\"https://itch.io/jam/mechint\">黑客松 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>中獲勝所提交的文章做的延伸。感謝倫敦<a class=\"link\" href=\"https://krischari.notion.site/Our-Coworking-Space-daff577809a84832a0d8bb28940c78c0\">EA Hub <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>讓我們使用他們的共享工作空間，感謝<a class=\"link\" href=\"https://fbarez.github.io/\">Fazl Barez的評論 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>和 <a class=\"link\" href=\"https://www.neelnanda.io/\">Neel Nanda的建議 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>，以及 <a class=\"link\" href=\"https://neuroscope.io/\">Neuroscope <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>、<a class=\"link\" href=\"https://huggingface.co/datasets/NeelNanda/pile-10k\">pile-10k資料集 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>和<a class=\"link\" href=\"https://github.com/neelnanda-io/TransformerLens\">TransformerLens <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>。</p>\n<hr>\n<div class=\"note-large notel-default\"><div class=\"notel-title\"><p>註解</p>\n</div><div class=\"notel-content\"><h5 id=\"註1\"><a href=\"#註1\" class=\"headerlink\" title=\"[註1]\"></a>[註1]</h5><p>Neel Nanda 對 MLP 第0層的看法：<br>“通常在GPT-2 Small上觀察到，MLP 第0層非常重要，抑制它會徹底破壞LLM的性能。我目前最好的猜測是，第一個MLP層基本上是 prompt 嵌入的擴展（無論出於什麼原因），當後續層想要訪問輸入 token 時，它們主要讀取的是第一個 ML 層的輸出，而不是 prompt。在這個框架下，第一個注意力層幾乎沒有什麼作用。<br>在這個框架下，MLP 第0層在第二個主題 token 上很重要，因為這是唯一一個位置有不同的輸入 token！<br>我不完全確定為什麼會這樣，但我的猜測是，這是因為在GPT-2 Small中，嵌入和去嵌入矩陣是相同的。這是相當不合理的，因為嵌入和去嵌入token的任務並不是互逆的，但這是常見的做法，模型可能希望將一些參數用於克服這一點。<br>我只有暗示性的證據，希望有人能夠好好研究這個問題”</p>\n<h5 id=\"註2\"><a href=\"#註2\" class=\"headerlink\" title=\"[註2]\"></a>[註2]</h5><p>神經元還有哪些可能的作用？它可能抑制了” a”的 logit，這對 logit 差異會產生相同的影響。或者它可能向殘差流中添加了完全不同的方向，導致後續層的神經元增加了” an” logit。</p>\n<h5 id=\"註3\"><a href=\"#註3\" class=\"headerlink\" title=\"[註3]\"></a>[註3]</h5><p>值得注意的是，” though”神經元與一組語義相似的token一致，而” an”神經元與一組語法相似的token相關（例如，” an”和” Ancients”）。</p>\n<h5 id=\"註4\"><a href=\"#註4\" class=\"headerlink\" title=\"[註4]\"></a>[註4]</h5><p>為什麼” an”具有更乾淨的相關性，儘管有其他一致的token(“an”, “An”, “ An”)？我們不能確定。一個可能的解釋是，”An”和” An”只是非常不常見的 token，所以它們對對相關性的影響很小，而”an”與此神經元的一致性遠低於其他三者。</p>\n<p>一般來說，我們只查看每個token的前2名神經元差異來找到的神經元，通常不會與其相對應的token具有乾淨的相關性，因為這些神經元可能同時與多個token有一致關係。</p>\n<h5 id=\"註5\"><a href=\"#註5\" class=\"headerlink\" title=\"[註5]\"></a>[註5]</h5><p>當我們觀察每個token的最一致神經元時，我們可以看到了一些非常高一致性的熟悉的<a class=\"link\" href=\"https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation\">麻煩製造者 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233886186-94185a3f-d7da-4c88-8a87-bdb14572dfe9.png\" alt=\"image\"></p>\n<p>起初，看起來這些”禁止的token”都與一個”禁止的神經元”（第35層神經元3354）相關，它們都與該神經元非常一致。但實際上，如果我們繪製許多其他神經元的最一致token，我們也會看到一些這樣的奇怪token位於排名靠前的位置。我們的初步假設是，這可能與<a class=\"link\" href=\"https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/?commentId=M2uAwsCus2adqQsGc\">hubness effect(集束效應) <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>有關。</p>\n<h5 id=\"註6\"><a href=\"#註6\" class=\"headerlink\" title=\"[註6]\"></a>[註6]</h5><p>Neuroscope 數據對於這個神經元並不可用，所以我們從 pile-10k 資料集中選擇了最大激發數據集案例。文本1、2、3分別是提示1755、8528和6375。</p>\n<p>[註7]<br>值得注意的是，前5個token之一是”an”，但這與我們之前提到的” an”不同，它很少作為一個單詞的開頭或單獨使用的單詞。同樣，與之配對的神經元，第34層神經元4549，也不是前面提到的” an”神經元。</p>\n </div></div>\n\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>本文翻譯自 <a class=\"link\" href=\"https://clementneo.com/posts/2023/02/11/we-found-an-neuron\">We Found An Neuron in GPT-2 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>，作者Clement Neo 是一位倫敦的大學生。這篇文章源自他在黑客松中的題目延伸，早在2月初我就發現這篇文章相當有趣並決定翻譯它，但開學後到現在才趁空檔完成。本文像一部偵探故事，逐步揭示LLM在處理特定token時所涉及的神經元，以及如何判定這些神經元是否確實負責預測相應的token。</p>\n</blockquote>\n<h2 id=\"我們在-GPT-2-中找到了”an”神經元\"><a href=\"#我們在-GPT-2-中找到了”an”神經元\" class=\"headerlink\" title=\"我們在 GPT-2 中找到了”an”神經元\"></a>我們在 GPT-2 中找到了”an”神經元</h2><p><code>作者：Joseph Miller、Clement Neo</code></p>\n<p>這個研究始於一個問題：<em><strong>GPT-2 是如何知道何時該使用 “an” 而不是 “a” ？</strong></em>以人類來說，這個選擇取決於後面的單詞是否以母音為開頭，但 GPT-2 一次只能輸出一個單詞（準確來說，1個 token），他是如何判斷的？</p>\n<p>雖然我們還沒有完整的答案，但我們確實在 GPT-2 Large 模型中找到了一個對於 gpt 預測 “ an” 這個 token 至關重要的單個 MLP 神經元。同時，我們也發現這個神經元的權重與 “ an” token 的嵌入相對應，這讓我們能夠以相同方法找到其他也能預測特定 token 的神經元。</p>\n<h3 id=\"發現神經元\"><a href=\"#發現神經元\" class=\"headerlink\" title=\"發現神經元\"></a>發現神經元</h3><h5 id=\"選擇提示詞-prompt\"><a href=\"#選擇提示詞-prompt\" class=\"headerlink\" title=\"選擇提示詞(prompt)\"></a>選擇提示詞(prompt)</h5><p>想出一個能讓 GPT-2 輸出 “ an”（前面的空格是token的一部分）作為最佳預測的提示(prompt)是非常困難。我們實驗後最後放棄了 GPT-2_small 模型，轉向 GPT-2_large。稍後我們將會看到，即使是 GPT-2_large 也會系統性地低估 “ an” 這個 token。這可能是因為較小的語言模型依賴於 “ a” 的頻率更高，更有可能做出最佳猜測。我們最終找到的能讓 “ an” 的機率達到 64% 的提示是：</p>\n<blockquote>\n<p>“I climbed up the pear tree and picked a pear. I climbed up the <code>apple</code> tree and picked”</p>\n</blockquote>\n<p>第一句是必要的，因為它可以讓模型朝著不定冠詞的預測方向前進，如果沒有第一句，模型會做出其他預測，例如: “[picked] up”</p>\n<p>在繼續之前，讓我們快速回顧一下 <a class=\"link\" href=\"https://transformer-circuits.pub/2021/framework/index.html\">Transformer 架構 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 。每個注意力區塊和 MLP 都會將輸入加到殘差流(residual stream)中。<br>   <img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232722119-09e06fca-baab-48e6-b887-4ca3c91cdd48.png\" alt=\"image\"></p>\n<h4 id=\"Logit-透鏡-Logit-Lens\"><a href=\"#Logit-透鏡-Logit-Lens\" class=\"headerlink\" title=\"Logit 透鏡(Logit Lens)\"></a>Logit 透鏡(Logit Lens)</h4><p>使用 <a class=\"link\" href=\"https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens\">logit 透鏡 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 這種技巧，我們在模型的每層之間從殘差流中取出 logit，並繪製 logit(“ an”) 和 logit(“ a”) 之間的差異。我們在第 31 層的 MLP 之後發現了一個大的峰值，如下圖。</p>\n<p>   <img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232725226-748c4121-8afe-49bf-8e03-49cd58f9d083.png\" alt=\"image\"></p>\n<h4 id=\"層次激發修補-Activation-Patching-by-the-Layer\"><a href=\"#層次激發修補-Activation-Patching-by-the-Layer\" class=\"headerlink\" title=\"層次激發修補(Activation Patching by the Layer)\"></a>層次激發修補(Activation Patching by the Layer)</h4><p>激發修補是一種由 <a class=\"link\" href=\"https://arxiv.org/abs/2202.05262\">Meng 等人 (2022) <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a> 提出的技術，用於分析 Transformer 中單層的重要性。在這個技術中，我們會先將一開始提到的提示運行於模型時，然後我們會將每一層的激發保存起來又稱“淨激發(clean activation)”。</p>\n<p>接著，我們將提示詞中蘋果樹(apple tree)改成檸檬樹(lemon tree)並丟給模型：</p>\n<blockquote>\n<p>“I climbed up the pear tree and picked a pear. I climbed up the <code>apple</code> tree and picked”</p>\n</blockquote>\n<blockquote>\n<p>“I climbed up the pear tree and picked a pear. I climbed up the <code>lemon</code> tree and picked”</p>\n</blockquote>\n<p>通過將 “apple” 替換為 “lemon”，我們誘使模型預測 “ a” 而非 “ an”。當模型預測 “ a” 而非 “ an” 時，我們可以將當前這層中的激活替換為其淨激活，以查看模型朝向 “ an” token移動的程度，這可以代表該前層對於預測 “ an” 的重要性。我們對模型中的所有層重複這個過程。</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232732236-84a664ab-c26f-430b-80b7-882819a83ba2.png\" alt=\"image\"></p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232732433-dd91af7e-3450-4b34-a1a9-76b3fa84d728.png\" alt=\"image\"></p>\n<p>這篇文章接下來我們不會談太多關於 Attention layer 的部分，但這些結果表明，第 26 層是 “ picked” 開始大量考慮 “ apple” 的地方，這顯然是預測 “ an” 所必需要具備的。</p>\n<p>注意：圖表上的刻度是相對的邏輯差異恢復程度(relative logit difference recovery)</p>\n<p><mjx-container class=\"MathJax\" jax=\"SVG\" display=\"true\"><svg style=\"vertical-align: -2.016ex;\" xmlns=\"http://www.w3.org/2000/svg\" width=\"43.239ex\" height=\"5.14ex\" role=\"img\" focusable=\"false\" viewBox=\"0 -1381 19111.4 2272\"><g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"scale(1,-1)\"><g data-mml-node=\"math\"><g data-mml-node=\"mfrac\"><g data-mml-node=\"mrow\" transform=\"translate(220,676)\"><g data-mml-node=\"mi\"><path data-c=\"1D443\" d=\"M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(751,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1280,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1641,0)\"><path data-c=\"1D450\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2074,0)\"><path data-c=\"210E\" d=\"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2650,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3116,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3636,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4317,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4802,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5279,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5624,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5985,0)\"><path data-c=\"1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6813,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(7158,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(7708,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(8480.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9480.4,0)\"><path data-c=\"1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10240.4,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10725.4,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11176.4,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11627.4,0)\"><path data-c=\"1D462\" d=\"M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12199.4,0)\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12702.4,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13063.4,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13529.4,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(14049.4,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(14730.4,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(15215.4,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(15692.4,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(16037.4,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(16398.4,0)\"><path data-c=\"1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(17226.4,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(17571.4,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(18121.4,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g></g><g data-mml-node=\"mrow\" transform=\"translate(711.5,-686)\"><g data-mml-node=\"mi\"><path data-c=\"1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(760,0)\"><path data-c=\"1D459\" d=\"M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1058,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(1524,0)\"><path data-c=\"1D44E\" d=\"M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2053,0)\"><path data-c=\"1D45B\" d=\"M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(2653,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3334,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(3819,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4296,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(4641,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5002,0)\"><path data-c=\"1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(5830,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6175,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(6725,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mo\" transform=\"translate(7497.2,0)\"><path data-c=\"2212\" d=\"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(8497.4,0)\"><path data-c=\"1D436\" d=\"M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9257.4,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(9742.4,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10193.4,0)\"><path data-c=\"1D45F\" d=\"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(10644.4,0)\"><path data-c=\"1D462\" d=\"M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11216.4,0)\"><path data-c=\"1D45D\" d=\"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(11719.4,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12080.4,0)\"><path data-c=\"1D452\" d=\"M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(12546.4,0)\"><path data-c=\"1D451\" d=\"M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13066.4,0)\"><path data-c=\"1D43F\" d=\"M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(13747.4,0)\"><path data-c=\"1D45C\" d=\"M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(14232.4,0)\"><path data-c=\"1D454\" d=\"M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(14709.4,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(15054.4,0)\"><path data-c=\"1D461\" d=\"M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(15415.4,0)\"><path data-c=\"1D437\" d=\"M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(16243.4,0)\"><path data-c=\"1D456\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(16588.4,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g><g data-mml-node=\"mi\" transform=\"translate(17138.4,0)\"><path data-c=\"1D453\" d=\"M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z\"></path></g></g><rect width=\"18871.4\" height=\"60\" x=\"120\" y=\"220\"></rect></g></g></g></svg></mjx-container></p>\n<p>(也就是說，圖代表有多少修補恢復原先提示中” an”和” a”之間的 logit 比例）</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/232733478-84b5de26-3ddc-42ca-ac8f-ea82d8bd2794.png\" alt=\"image\"></p>\n<p>兩個最突出的 MLP 層是第 0 層和第 31 層。我們已經知道第 0 層的 MLP 對於 GPT-2 的運作非常重要（儘管我們不確定為什麼第 0 層的注意力很重要）[註1]。第 31 層的效果更有趣。我們的結果表明，第 31 層的 MLP 在預測 “ an” token 方面有著顯著的作用。（如果你對此結果如何與上面的 logit 透鏡協作感到困惑，你可以參考此處<a class=\"link\" href=\"https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2?commentId=FLpxtfnwnMjZwXv3B#comments\">評論 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>)</p>\n<h3 id=\"發現一：-我們可以通過對單個神經元進行激發修補-activation-patching-來發現具有預測性的神經元\"><a href=\"#發現一：-我們可以通過對單個神經元進行激發修補-activation-patching-來發現具有預測性的神經元\" class=\"headerlink\" title=\"發現一： 我們可以通過對單個神經元進行激發修補(activation patching)來發現具有預測性的神經元\"></a>發現一： <br>我們可以通過對單個神經元進行激發修補(activation patching)來發現具有預測性的神經元</h3><p>激發修補(activation patching)已被用來按層研究 Transformer ，但我們能否將這種技術推向更遠，將其應用到單個神經元？由於 Transformer 中的每個 MLP 只有一個隱藏層，因此每個神經元的激活都不會影響 MLP 中的其他神經元。所以，我們應該能夠對單個神經元進行修補，因為它們在同一層的 Attention heads 之間是相互獨立的。</p>\n<p>我們以與上述提到的分層修補中類似的方法對第 31 層 MLP 的神經元進行神經元級別的激發修補(activation patching)。當我們將被修改過的提示運行於模型時，在 MLP 中重新引入每個神經元的淨激發(clean activation)，並觀察恢復每個神經元對 “ a” 和 “ an” 之間 logit 差異的貢獻。</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233306391-41c0f3f6-7dec-4b63-b9f7-48fb11d7c492.png\" alt=\"image\"></p>\n<p>我們可以發現修補神經元 892 可以恢復最初提示中 50% 的 logit 差異，而修補整個層實際上只恢復了 49% 的表現上更差。</p>\n<p>總之，我們在 GPT-2 Large 中找到了一個對預測 “ an” token 至關重要的單個 MLP 神經元。通過啟動修補技術，我們可以繼續研究變換器中單個神經元的重要性，進一步了解它們在預測特定 token 方面的作用。雖然我們尚未完全解答開篇提出的問題，但這些發現將有助於我們更深入地理解語言模型的運作機制。</p>\n<h3 id=\"發現二：「an-神經元」的激發與「an」token-預測相關性\"><a href=\"#發現二：「an-神經元」的激發與「an」token-預測相關性\" class=\"headerlink\" title=\"發現二：「an-神經元」的激發與「an」token 預測相關性\"></a>發現二：「an-神經元」的激發與「an」token 預測相關性</h3><h4 id=\"Neuroscope-第31層的892號神經元最大激發案例\"><a href=\"#Neuroscope-第31層的892號神經元最大激發案例\" class=\"headerlink\" title=\"Neuroscope 第31層的892號神經元最大激發案例\"></a>Neuroscope <a class=\"link\" href=\"https://neuroscope.io/gpt2-large/31/892.html\">第31層的892號神經元 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>最大激發案例</h4><p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233309645-a7705dd3-c1c6-449d-a39c-0690357067b6.png\" alt=\"image\"><br>Neuroscope 是用來查看 GPT-2 中每個神經元在大型資料集中最大激發案例的線上工具。當我們觀察第31層，編號892神經元時，我們發現這個神經元在後續 token 是「an」的情況下達到最大的激發(maximally activates)。</p>\n<p>但 Neuroscope 只顯示出最高激發的前 20 個範例。我們想知道的是在更廣泛的激發範圍內，神經元與 token 是否存在相關性？</p>\n<h4 id=\"在更大的數據集上測試神經元\"><a href=\"#在更大的數據集上測試神經元\" class=\"headerlink\" title=\"在更大的數據集上測試神經元\"></a>在更大的數據集上測試神經元</h4><p>為了確認這一點，我們將 pile-10k 資料集運行在模型中。這是一個多樣化的資料集，包含了大約 1000 萬個來自 The Pile 的標記，分成 1024 個標記的提示。我們繪製了神經元激活範圍內「an」預測的比例：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233310982-62139036-2b3f-4a48-9d1a-4b46b65b79ed.png\" alt=\"image\"><br>我們看到「an」 token 的預測率隨著神經元激發數增加而增加，甚至到達總是最高預測的程度。趨勢中可以看出有些噪聲，這暗示模型中可能還有其他機制也對「an」的預測有所影響。或者當「an」的 logit 增加時，其他 logit 也會同時增加。</p>\n<p>值得注意的是，儘管資料集中實際出現了 12,000 次「an」，但模型只預測了 1,500 次「an」。難怪找到一個好的提示這麼困難！</p>\n<h4 id=\"神經元的輸出權重-output-weights-與「an」token-的內積很高\"><a href=\"#神經元的輸出權重-output-weights-與「an」token-的內積很高\" class=\"headerlink\" title=\"神經元的輸出權重(output weights)與「an」token 的內積很高\"></a>神經元的輸出權重(output weights)與「an」token 的內積很高</h4><p>神經元如何影響模型的輸出？有趣的是，神經元的輸出權重與「an」token 的嵌入有很高的內積。我們將這個內積稱為這個神經元與 token 的一致性(congruence)。與其他token相比（如「any」和「had」），神經元的一致性在「an」這個token中表現的非常高：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233312469-290d6f35-d93f-478d-9597-1ad12848c5ef.png\" alt=\"image\"><br>事實上，當我們計算神經元與所有 token 的一致性(congruence)時，有一些明顯的異常值：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233313512-0a4b2991-f948-48c4-924e-0d0458fdd686.png\" alt=\"image\"><br>看起來這個神經元基本上是將「an」的嵌入添加到殘差流(residual stream)中，這增加了「an」的輸出機率，因為移除嵌入的步驟包括將最終殘差與每個token的內積相乘。[註2]</p>\n<p>還有其他神經元也與「an」一致嗎？為了找出答案，我們繪製了所有神經元與「an」標記的一致性圖：<br><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233314247-d93a0938-8b36-4992-be48-6cd93076e922.png\" alt=\"image\"></p>\n<p>我們的神經元(31-892)的一致性遠高於其他神經元，但還有一些其他神經元具有相當高的一致性(congruence)。這些其他神經元可能是為什麼「an」神經元激發時與「an」token 預測之間相關性不完美的原因之一：有可能存在其他提示(prompt)有著「an」，而模型是使用其他神經元來完成該提示的「an」token。</p>\n<p>如果是這樣的話，我們能否使用一致性來找到與單個 token 預測有完美相關性的神經元？</p>\n<h3 id=\"發現三：我們可以使用神經元的輸出一致性來找到預測-token-的特定神經元\"><a href=\"#發現三：我們可以使用神經元的輸出一致性來找到預測-token-的特定神經元\" class=\"headerlink\" title=\"發現三：我們可以使用神經元的輸出一致性來找到預測 token 的特定神經元\"></a>發現三：我們可以使用神經元的輸出一致性來找到預測 token 的特定神經元</h3><h4 id=\"找到與特定-token-相關的神經元\"><a href=\"#找到與特定-token-相關的神經元\" class=\"headerlink\" title=\"找到與特定 token 相關的神經元\"></a>找到與特定 token 相關的神經元</h4><p>我們可以透過以下步驟搜索來找到與特定 token 相關的神經元：</p>\n<ol>\n<li>對於每個 token，找到具有最高輸出一致性(congruence)的神經元</li>\n<li>對於這其中的每一個神經元，比較預測同一個 token 中擁有最高一致性和第二高的神經元</li>\n<li>找到擁有 最獨特一致性(the most exclusively congruent)的神經元。<br>通過這種搜索，我們希望找到對特殊 token 具有影響力的神經元。我們的猜想是，這些神經元的激發與其 token 的預測更相關，因為該 token 的任何預測都會「依賴」於該神經元。</li>\n</ol>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233317704-3d37d708-ef82-4249-a080-0b7257ff539e.png\" alt=\"image\"></p>\n<p>讓我們試著跑一次「though」神經元 - 第 28 層神經元 1921 - 看我們是否能得到更乾淨的統計圖<br><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233318279-2d83bf69-9dcd-4831-86d3-18410b827a08.png\" alt=\"image\"></p>\n<p>哇，這比” an”神經元的圖要亂多了。發生了什麼事？</p>\n<p>我們來看看 Neuroscope 提供的神經元數據，發現這個神經元負責預測了” though”和” however”這兩個token。這讓事情變得更複雜，因為看來這個神經元是與一組語義相似的 token（連接副詞）相關。<br>[註3]<br><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319211-2922b2fc-230d-4c48-b0eb-142eb88e5b53.png\" alt=\"image\"></p>\n<p>當我們計算神經元與所有 token 的一致性時，我們發現類似的 token 經常作為異常值出現：<br><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319375-96407240-e7b3-4544-a0e8-645b4f8dc136.png\" alt=\"image\"></p>\n<p>在上面的大型數據集相關圖中，神經元激發並且預測” however”這個 token 的一致性被預測” though” token 超過的狀況是一個負面的案例，因為” though” 不是這個神經元最高預測 token 。這也可以解釋” an”相關中的一些噪聲，因為與”An”、” An”和”an”具有相同的意思，這有可能是噪聲的來源。[註4]</p>\n<p>我們能找到一個更簡單的神經元來研究嗎？最好是一個只預測單個 token 的神經元。</p>\n<h4 id=\"找到一個乾淨的關聯神經元\"><a href=\"#找到一個乾淨的關聯神經元\" class=\"headerlink\" title=\"找到一個乾淨的關聯神經元\"></a>找到一個乾淨的關聯神經元</h4><p>要使神經元與 token “乾淨地關聯”，它們之間的一致性應該是互斥的，這意味著：</p>\n<ol>\n<li>神經元與token的一致性遠大於與其他任何神經元的一致性。</li>\n<li>神經元與token的一致性遠大於與其他任何token的一致性。<br>（請記住，”一致性(congruence)”僅是我們表達內積的術語。）</li>\n</ol>\n<p>這兩個標準都有助於簡化神經元與其 token 之間的關係。如果神經元與 token 的一致性是表示神經元對該 token 預測的貢獻程度，第一條規則是確保「只有這個神經元」負責預測該token，而第二個規則則可以確保這個神經元「只會負責預測該token」。</p>\n<p>我們的搜索如下：</p>\n<ol>\n<li><strong>對於每個 token，找到最一致的神經元。</strong></li>\n<li><strong>對於每個神經元，找到最一致的 token。</strong>[註5]</li>\n<li><strong>找到兩個列表上的token神經元對，即神經元最一致的token是與該神經元最一致的token！</strong></li>\n<li><strong>通過將它們的前兩個token一致性差與前兩個神經元一致性差相乘，計算它們之間的差異程度。</strong></li>\n<li><strong>找到具有最高互斥一致性的對。</strong></li>\n</ol>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319494-b978d131-5e9d-4858-b3ae-b254692bbd12.png\" alt=\"image\"><br>對於GPT-2_large，第33層神經元4142與”i”配對在這個度量上得分最高。查看 Neuroscope [註6]確認了這個關聯：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319606-3e8f5dc0-43cf-47cb-aecd-33dd83d7334e.png\" alt=\"image\"><br>當我們繪製前5名最高得分者的最高預測比例與激發圖時：[註7]</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233319772-b1fafa3e-eb7f-4379-8abb-adcdd0065ada.png\" alt=\"image\"><br>我們確實看到了每對之間的強相關性！</p>\n<h3 id=\"這一切意味著什麼？\"><a href=\"#這一切意味著什麼？\" class=\"headerlink\" title=\"這一切意味著什麼？\"></a>這一切意味著什麼？</h3><p>神經元與token的一致性是否真的衡量了神經元預測該token的程度？我們不知道。即使具有高互斥一致性的token神經元對也可能並不總是具有相關性，原因可能有幾個：</p>\n<ol>\n<li>token也可能由一組一致性較低的神經元共同預測。</li>\n<li>token可能由注意力頭（attention heads）預測。</li>\n<li>即使神經元的激發與 token 的 logit 具有高度相關性，它也可能間接地與其他 token 的 logit 相關，以至於神經元的激發與 token 出現的機率不具有相關性。</li>\n<li>可能有後續層往殘差流中添加相反方向，從而抵消神經元的效果。</li>\n</ol>\n<p>雖然是這樣說，但我們發現具有最高互斥一致性的前5名的神經元確實對上 token 預測具有很強的相關性。</p>\n<h3 id=\"TL-DR\"><a href=\"#TL-DR\" class=\"headerlink\" title=\"TL; DR\"></a>TL; DR</h3><ol>\n<li>我們在神經元級別上使用激發修補（activation patching）找到了一個對於預測特定提示中的token “ an”非常重要的神經元。</li>\n<li>“ an”神經元的激發與” an” token 的預測有一般相關性。</li>\n<li>這可能是因為神經元的輸出權重與” an” token具有很高的內積（神經元與 token 有高度一致性）。此外，這個神經元與這個 token 的內積比其他任何 token 都要高。而且，這個 token 與這個神經元的內積比 token 與其他任何神經元的內積都要高（它們具有高度互斥的一致性）。</li>\n<li>神經元與 token 之間的一致性很酷。我們找到了前5名具有互斥一致性的 神經元-token 對。這些神經元的激發與它們各自的 token 預測密切相關。</li>\n</ol>\n<p>重現我們結果的程式碼：<a class=\"link\" href=\"https://github.com/UFO-101/an-neuron\">https://github.com/UFO-101/an-neuron <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a></p>\n<p>這是我們在<a class=\"link\" href=\"https://apartresearch.com/\">Apart Research <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>的<a class=\"link\" href=\"https://itch.io/jam/mechint\">黑客松 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>中獲勝所提交的文章做的延伸。感謝倫敦<a class=\"link\" href=\"https://krischari.notion.site/Our-Coworking-Space-daff577809a84832a0d8bb28940c78c0\">EA Hub <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>讓我們使用他們的共享工作空間，感謝<a class=\"link\" href=\"https://fbarez.github.io/\">Fazl Barez的評論 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>和 <a class=\"link\" href=\"https://www.neelnanda.io/\">Neel Nanda的建議 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>，以及 <a class=\"link\" href=\"https://neuroscope.io/\">Neuroscope <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>、<a class=\"link\" href=\"https://huggingface.co/datasets/NeelNanda/pile-10k\">pile-10k資料集 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>和<a class=\"link\" href=\"https://github.com/neelnanda-io/TransformerLens\">TransformerLens <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>。</p>\n<hr>\n<div class=\"note-large notel-default\"><div class=\"notel-title\"><p>註解</p>\n</div><div class=\"notel-content\"><h5 id=\"註1\"><a href=\"#註1\" class=\"headerlink\" title=\"[註1]\"></a>[註1]</h5><p>Neel Nanda 對 MLP 第0層的看法：<br>“通常在GPT-2 Small上觀察到，MLP 第0層非常重要，抑制它會徹底破壞LLM的性能。我目前最好的猜測是，第一個MLP層基本上是 prompt 嵌入的擴展（無論出於什麼原因），當後續層想要訪問輸入 token 時，它們主要讀取的是第一個 ML 層的輸出，而不是 prompt。在這個框架下，第一個注意力層幾乎沒有什麼作用。<br>在這個框架下，MLP 第0層在第二個主題 token 上很重要，因為這是唯一一個位置有不同的輸入 token！<br>我不完全確定為什麼會這樣，但我的猜測是，這是因為在GPT-2 Small中，嵌入和去嵌入矩陣是相同的。這是相當不合理的，因為嵌入和去嵌入token的任務並不是互逆的，但這是常見的做法，模型可能希望將一些參數用於克服這一點。<br>我只有暗示性的證據，希望有人能夠好好研究這個問題”</p>\n<h5 id=\"註2\"><a href=\"#註2\" class=\"headerlink\" title=\"[註2]\"></a>[註2]</h5><p>神經元還有哪些可能的作用？它可能抑制了” a”的 logit，這對 logit 差異會產生相同的影響。或者它可能向殘差流中添加了完全不同的方向，導致後續層的神經元增加了” an” logit。</p>\n<h5 id=\"註3\"><a href=\"#註3\" class=\"headerlink\" title=\"[註3]\"></a>[註3]</h5><p>值得注意的是，” though”神經元與一組語義相似的token一致，而” an”神經元與一組語法相似的token相關（例如，” an”和” Ancients”）。</p>\n<h5 id=\"註4\"><a href=\"#註4\" class=\"headerlink\" title=\"[註4]\"></a>[註4]</h5><p>為什麼” an”具有更乾淨的相關性，儘管有其他一致的token(“an”, “An”, “ An”)？我們不能確定。一個可能的解釋是，”An”和” An”只是非常不常見的 token，所以它們對對相關性的影響很小，而”an”與此神經元的一致性遠低於其他三者。</p>\n<p>一般來說，我們只查看每個token的前2名神經元差異來找到的神經元，通常不會與其相對應的token具有乾淨的相關性，因為這些神經元可能同時與多個token有一致關係。</p>\n<h5 id=\"註5\"><a href=\"#註5\" class=\"headerlink\" title=\"[註5]\"></a>[註5]</h5><p>當我們觀察每個token的最一致神經元時，我們可以看到了一些非常高一致性的熟悉的<a class=\"link\" href=\"https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation\">麻煩製造者 <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>：</p>\n<p><img lazyload=\"\" src=\"/images/loading.svg\" data-src=\"https://user-images.githubusercontent.com/89479282/233886186-94185a3f-d7da-4c88-8a87-bdb14572dfe9.png\" alt=\"image\"></p>\n<p>起初，看起來這些”禁止的token”都與一個”禁止的神經元”（第35層神經元3354）相關，它們都與該神經元非常一致。但實際上，如果我們繪製許多其他神經元的最一致token，我們也會看到一些這樣的奇怪token位於排名靠前的位置。我們的初步假設是，這可能與<a class=\"link\" href=\"https://www.lesswrong.com/posts/Ya9LzwEbfaAMY8ABo/?commentId=M2uAwsCus2adqQsGc\">hubness effect(集束效應) <i class=\"fa-regular fa-arrow-up-right-from-square fa-sm\"></i></a>有關。</p>\n<h5 id=\"註6\"><a href=\"#註6\" class=\"headerlink\" title=\"[註6]\"></a>[註6]</h5><p>Neuroscope 數據對於這個神經元並不可用，所以我們從 pile-10k 資料集中選擇了最大激發數據集案例。文本1、2、3分別是提示1755、8528和6375。</p>\n<p>[註7]<br>值得注意的是，前5個token之一是”an”，但這與我們之前提到的” an”不同，它很少作為一個單詞的開頭或單獨使用的單詞。同樣，與之配對的神經元，第34層神經元4549，也不是前面提到的” an”神經元。</p>\n </div></div>\n\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"clg4v1d7x0000fzx1cphv6mgp","tag_id":"clg4v1hl10001fzx1eph87gyh","_id":"clg4v1hl20004fzx174h69sqy"},{"post_id":"clg4v1d7x0000fzx1cphv6mgp","tag_id":"clg4v1hl20002fzx13o7aeye8","_id":"clg4v1hl20005fzx1bv7sgm4t"},{"post_id":"clg4v1d7x0000fzx1cphv6mgp","tag_id":"clg4v1hl20003fzx123yi3d0v","_id":"clg4v1hl20006fzx1czwsdfow"},{"post_id":"clgugpjbs0000c1x196n3cgv3","tag_id":"clgm0msfp0001scx14roqgh10","_id":"clgugpjbs0001c1x1b6ga3els"},{"post_id":"clgugpjbs0000c1x196n3cgv3","tag_id":"clg4v1hl20002fzx13o7aeye8","_id":"clgugpjbs0002c1x18ri2c0p2"},{"post_id":"clgugpjbs0000c1x196n3cgv3","tag_id":"clhhaez0r0000z5ur1mviath2","_id":"clhhaez0s0003z5urf6ly2a6g"},{"post_id":"clgugpjbs0000c1x196n3cgv3","tag_id":"clhhaez0s0001z5ur40hogk29","_id":"clhhaez0t0004z5urhedygitu"},{"post_id":"clgugpjbs0000c1x196n3cgv3","tag_id":"clhhaez0s0002z5urahk37r1g","_id":"clhhaez0t0005z5urc9f6bsw8"}],"Tag":[{"name":"ChatGPT Discord 機器人","_id":"clg4v1hl10001fzx1eph87gyh"},{"name":"ChatGPT","_id":"clg4v1hl20002fzx13o7aeye8"},{"name":"Discord 機器人","_id":"clg4v1hl20003fzx123yi3d0v"},{"name":"gpt","_id":"clgm0msfp0001scx14roqgh10"},{"name":"gpt-2","_id":"clhhaez0r0000z5ur1mviath2"},{"name":"類神經網路","_id":"clhhaez0s0001z5ur40hogk29"},{"name":"大型語言模型","_id":"clhhaez0s0002z5urahk37r1g"}]}}